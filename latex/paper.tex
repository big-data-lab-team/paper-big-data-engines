\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{times}
\usepackage[binary-units=true]{siunitx}
\usepackage{latexsym}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=blue,filecolor=black,urlcolor=blue}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{caption}
\usepackage{subcaption}

%\setlength{\textfloatsep}{7pt}

\newcommand{\TG}[1]{\color{cyan}From Tristan: #1 \color{black}}
\newcommand{\MD}[1]{\color{magenta}From Mathieu: #1 \color{black}}
\newcommand{\VHS}[1]{\color{green}From Valerie: #1 \color{black}}

\begin{document}

\title{Performance comparison of Dask and Apache Spark on HPC systems}

\author{Mathieu Dugr\'e, Val\'erie Hayot-Sasson, Tristan Glatard\\
	Department of Computer Science and Software Engineering\\
	Concordia University, Montr\'eal, Qu\'ebec, Canada\\
	\{mathieu.dugre, valerie.hayot-sasson, tristan.glatard\}@concordia.ca
	\vspace*{0.8cm} % to avoid weird spacing of 1st page by Latex.
}

\maketitle

\begin{abstract}
	% TODO
\end{abstract}

\begin{IEEEkeywords}
	Performance, Big Data, Dask, Spark, Neuroimaging
\end{IEEEkeywords}
\MD{Discuss which keyword to use for the paper.}

\section{Introduction}
The raise in data sharing coupled with improved data collection technologies lead neuroimaging into the Big Data era\cite{ALFAROALMAGRO:18, van2014human}. \MD{cite CONP portal}
While common neuroimaging workflow engines, such as Nipype\cite{Nipype:11}, are well rounded to process standard compute-intensive pipeline,
they lack Big Data strategies (i.e. in-memory computing, data locality, and lazy-evaluation) to improve the performance of increasingly prevalent data-intensive pipelines.
A previous study\cite{hayot2019performance} notes the importantce of in-memory computing and data locality to improve the performance of data-intensive pipelines.
In this work, we study how performance benchmarks generalize accross Big Data engines in an HPC environement.
In particular, we study the performance difference between Dask\cite{Dask:15} and Apache Spark\cite{Spark:16}, for their sutability to process neuroimaging pipelines.
Our objective is to assess whether Spark or Dask has substantial performance benefits to process data-intensive pipeline in HPC clusters.

Spark and Dask both provide in-memory computing, data locality, and lazy-evlaution, which is common for Big Data engines.
Both of their scheduler operate dynamically, which benefit applications with runtime unknown ahead of time\cite{Dask:15}.
They also provide rich high-level API and support a variety of scyeduler and deployment environment, such as Mesos\cite{hindman2011mesos}, YARN\cite{vavilapalli2013apache}, Kubernetes, and HPC clusters.
Although sharing similarities, these engines have differences.

% Overview of dask and spark
First and foremost, Spark is written in Scala while Dask is in Python.
Given the popularity of Python in the scientific community, this arguably gives a edge to Dask due to the serialization cost between Python and Scala.
However, if not careful, the Python Global Intepreter Lock (GIL) can significatly reduce the parallelism of an application, in some cases.
The difference in programmming language also provides different benefits.
On the one hand, as part of the Scipy ecosystem, Dask provides almost transparent integration with APIs such as Numpy arrays, Pandas dataframe, or RAPIDS GPU accelerated code framework.
On the other hand, Spark's Java, Scala, R, and Python APIs allow to easily parallelize pipelines with minimal performance loss; thanks to the JVM.
Our study is restrained to performance, although, we recognize that, in practice, other factors affect the choice of an engine.

% Why we chose Lustre TODO
While laptops or workstations can be sufficient for some applications, data-intensive neuroimaging pipelines often require large infrastructure to perform studies.
This paper focuses on the performance of Big Data engine in HPCs environments.

% Related work TODO

The next section introduce the Lustre file system and both Big Data engines used: Dask and Apache Spark.
Following, we present the design of our benchmarking experiments.
We consider two type of data-intensive neuroimaging pipelines: high-resolution imaging and large functional MRI studies.
The pipelines we chose involve different compute and I/O patterns; e.g. map-reduce or map-only and requiring partial data in memory or the whole dataset.
Our experiments were executed on a dedicated cluster representative of an HPC environment used to process state-of-the-art data-intensive neuroimaging studies.
Further sections present our results, dicsussion, and concusion.


\section{Background}
\subsection{Lustre}
% TODO

\subsection{Dask}
Dask is a Python-based Big Data engine with growing popularity in the scientific Python ecosystem.
Dask was designed with data locality and in-memory computing in mind, to mitigate the data transfer bottleneck in Big Data workflows.
Data locality, popularized by Map-Reduce \cite{dean2008mapreduce}, schedules tasks where the data reside.
In-memory computing minimizes the overhead of transferring data to disk by keeping data in memory when possible.
Dask uses lazy evaluation to reduce unnecessary communication and computation.
The engine builds a dynamic graph before execution, allowing it to determine which task to compute.
Dask workflows can further reduce data transfer by leveraging multithreading whenever Python's GIL does not restrict it.
Fault-tolerance is achieved by recording data lineage: the sequence of operations used to modify the initial data.

Dask offers five data structures:
\href{https://docs.dask.org/en/latest/array.html}{Array},
\href{https://docs.dask.org/en/latest/bag.html}{Bag},
\href{https://docs.dask.org/en/latest/dataframe.html}{DataFrame},
\href{https://docs.dask.org/en/latest/delayed.html}{Delayed},
and \href{https://docs.dask.org/en/latest/futures.html}{Futures}.
Arrays offer a clone of NumPy API for distributed processing of large arrays.
Bags are a distributed collection of Python object that offers a programming abstraction similar to \href{https://toolz.readthedocs.io/en/latest/}{PyToolz}.
Dataframes are a parallel composition of \href{https://pandas.pydata.org/}{Pandas} Dataframes used to process a large amount of tabular data.
Dask Delayed offers an API for distributing arbitrary functions that do not fit in the above frameworks.
Lastly, Dask Futures can also execute arbitrary functions; however, it launches computation immediately rather than lazily.
Dask modularity allows users to install only required components making it lightweight.

In Dask, a scheduler decides where and when to execute tasks using the Dask graph.
API operations generate multiple fine-coarse tasks in the computation graph, allowing a more straightforward representation of complex algorithms.

The Dask engine is compatible with multiple distributed schedulers, including YARN and Mesos.
Dask also provides its own \textit{Dask Distributed scheduler}.
We chose to use Dask Distributed scheduler to keep the environment balanced between the engines.

In the Dask Distributed scheduler, a \textit{dask-scheduler} process administrates the resource provided by  \textit{dask-worker}s in the cluster.
The scheduler receives jobs from clients and assigns tasks to available workers.
Task scheduler uses a LIFO (Last-In-First-Out) job scheduling policy.
That is an utter process branch of the Dask graph before proceeding with the next one.

Dask offers multiple ways to deploy a cluster, including, but not limited to, SSH configs, Kubernetes, SLURM, PBS.
For our experiments, we used the \href{https://jobqueue.dask.org/en/latest/generated/dask_jobqueue.SLURMCluster.html}{Dask SLURM cluster} API.

\subsection{Apache Spark}
Apache Spark is a widely-used general-purpose Big Data engine.
Like Dask, it aims at reducing data transfer costs by incorporating data locality, in-memory computing, and lazy evaluation.

Spark offers three options to schedule jobs: Spark Standalone, Mesos, and YARN.
Spark Standalone is a simple built-in scheduler.
YARN is mainly used to schedule Hadoop-based workflows, while Mesos can be used for various workflows.
We limit our focus to Spark Standalone scheduler, as researchers are likely to execute their workflows in an HPC environment where, usually, neither YARN nor Mesos is available.

In the Spark Standalone scheduler, a \textit{leader} \TG{I am all in favor of inclusive terminology, but if Spark didn't update their wording we shouldn't do it for them} coordinates the resource provisioned by \textit{workers} in the cluster.
A \textit{driver}  process receives jobs from clients and requests workers from the leader.
Jobs are divided into stages to be executed onto workers.
Each operation in a stage is represented by a high-level task in the computation graph.
Like Dask, Spark Standalone scheduler uses a LIFO policy to schedule tasks.
Spark Standalone has two execution modes: (1) the client mode, where the driver process runs in a dedicated process,
and (2) the cluster mode, where the driver runs within a worker process.
Our experiments use the client mode since cluster mode is not available in PySpark.

Spark's primary data structure is Resilient Distributed Dataset (RDD)\cite{RDD}, a fault-tolerant, parallel collection of data elements.
RDDs are the basis of the other Spark data structure: Datasets and DataFrames.
Datasets are similar to RDD but benefit additional performance by leveraging the Spark SQL's optimized execution engine. 
The DataFrames are Datasets organized into named-columns and are used to process tabular data. 
While the DataFrame API is available in all supported languages, Datasets are limited to Scala and Java. 

Python is a standard programming language in the scientific community, offering numerous data processing libraries.
While serialization from Python to Java, an operation required when using Spark's Python API, creates overhead, we found it minimal \cite{8943502}.
We focus on PySpark API to have a more balanced environment between the different engines and for its suitability to neuroimaging.

\section{Methods}
\subsection{Infrastructure}
For our experiments we used a dedicated cluster.
Each compute node has 2 16-cores CPUs (64 threads total),
\SI{256}{\giga\byte} \SI{2666}{\mega\hertz} memory in 8-channel,
and \SI{2.88}{\tera\byte} SSDs of mounted storage.
The nodes are interconnected with a dedicated \SI{10}{\giga\bit/\second} Ethernet connection.
Centos 8 with Linux kernel \textit{4.18.0-240.1.1.el8\_lustre.x86\_64} installed on the compute nodes.

Both Spark and Dask are configured to have 8 worker processes each with 8 threads.
Each worker is allocated \SI{31.5}{\giga\byte} of memory.
A new cluster is spinned up and teared down for each experiment.

Dask ?? and Spark ?? \MD{Add version when after locking it for experiments.} was used for our experiments.

\subsection{Dataset}
We used BigBrain\cite{Amunts:13}, a 3-D image of the human brain with voxel intensity ranging between 0 and 65,535.
We converted the blocks into the NIfTI format, a popular format in neuroimaging.
We left the NIfTI blocks uncompressed, resulting in a total data size of \SI{648}{\giga\byte}.
To evaluate the effect of block size, we resplit these blocks into 1000, 2500, and 5000 blocks of \SI{648}{\mega\byte}, \SI{259.2}{\mega\byte}, and \SI{129.6}{\mega\byte}, respectively.
\href{https://github.com/big-data-lab-team/sam}{sam} library was used to resplit the image.
	
We also used the dataset provided by the Consortium for Reliability and Reproducibility (\href{http://fcon_1000.projects.nitrc.org/indi/CoRR/html/}{CoRR}) \cite{zuo2014open}, freely available on \href{https://datasets.datalad.org/?dir=/corr/RawDataBIDS}{datalad}.
The entire dataset is \SI{408.4}{\giga\byte}, containing anatomical, diffusion and functional images of 1,397 subjects acquired in 29 sites.
We used all 3,491 anatomical images, representing \SI{39}{\giga\byte} overall (\SI{11.17}{\mega\byte} per image on average).
	
\subsection{Applications}
\subsubsection{Increment}
We adapted the increment application used in \cite{hayot2019performance}.
This synthetic application reads blocks of the BigBrain from Lustre and simulates computation by sleeping for a specified period.
To simulate intermediate results, we repeat the sleep process for a configurable amount of time.
We prevent data caching of the blocks by incrementing their voxels value by one after each sleep operation.
Finally, we write the resulting NIfTI image back to Lustre.
Using this application we study the engines when their inputs are processed independently.
The map-only scenario of this application mimics the processing of multiple independent subjects in parallel.


\begin{figure}[!ht]
	\centering
	\includegraphics[height=\columnwidth,
	angle=0]{figures/increment.png}
	\caption{Task graph for Incrementation with 3 iterations and 3 BigBrain blocks.}
	\label{fig:graph-increment}
\end{figure}

% Currently ommitted due to issue with memory.
% 
\subsubsection{Multi-Increment}
Our second application is an adaptation of the increment application.
A significant difference is that, at each iteration, it uses a random BigBrain block as the increment value. 
This change allows the multi-increment application to have inter-worker communication while remaining simple.

\begin{figure}[!hb]
	\centering
	\includegraphics[height=\columnwidth,
	angle=0]{figures/multi-increment.png}
	\caption{Task graph for Multi-Incrementation with 3 iterations and 3 BigBrain blocks.}
	\label{fig:graph-muti-increment}
\end{figure}
	
\subsubsection{Histogram}
As our third application, we calculate the histogram of the BigBrain image. 
The application reads the BigBrain blocks from Lustre, calculates each intensity's frequency, and then writes the aggregated result back on Lustre.
This map-reduce application has a very high read overwrite ratio.
Moreover, this application requires shuffling, albeit of a limited amoutn of data. 
The amount of inter-worker communication is in-between the increment and multi-increment applications.

\begin{figure}[!hb]
	\centering
	\includegraphics[height=\columnwidth,
	angle=0]{figures/histogram.png}
	\caption{Task graph for Histogram with 3 BigBrain blocks.}
	\label{fig:graph-histogram}
\end{figure}
	
\subsubsection{Kmeans}
For our fourth application, we apply Kmeans clustering to the voxel intensities of the BigBrain image.
We set the number of clusters to 3, to segment the white and grey matter and the noise.
The application starts by reading the image blocks, combining all voxels in a 1-D array, and choosing initial centroids using the min, max, and intermediate values.
It assigns each voxel to the centroids it is the closest and updates each centroid by computing the average of the voxels associated with it. 
It repeats the assignment and update steps for a configurable amount of time. 
Finally, the voxels of the image blocks are classified and written back to the file system.
Updating the centroids involves substantial data communication between the workers.

For this application, the Spark and Dask implementations differ slightly,  to take advantage of the best-suited API from both engines.
The Spark implementation uses the Map-Reduce paradigm, while the Dask one uses array programming.
	
	
\subsubsection{BIDS App example}
Our fifth application is BIDS App example: a neuroimaging pipeline to measure the brain volume from MRIs.
For this application, we use the CoRR dataset.
The application extracts the brain volume of each participant, then computes the average for each group of participants.
Unlike the other applications, BIDS App example is a command-line executed in a Docker image (bids/example on DockerHub).
We converted the Docker image to a Singularity image for use in HPC environments, \MD{Cite paper on reason why this is done.}
using \href{https://hub.docker.com/r/singularityware/docker2singularity/tags/}{docker2singularity}

\subsection{Experiments}
Table~\ref{table:parameters} the four parameters that are varied throughout the experiments.
We varied (1) the number of workers to assess the scalibility of the scheduler for the engine,
(2) the BigBrain block size in Increment, Multi-Increment, and Histogram to measure the effect of the different I/O pattern and parallelization degrees,
(3) the number of iterations to evaluate the effect of number of task,
and (4) the sleep delay to study the effect of task duration.
It should note that increasing the number of iterations for a given sleep delay also increases the total compute time of an application.

To avoid potential external bias such as caching, background process and network load, we ran the applications in randomized order and cleared the page cache in between every experiments.
Each benchmark was run ten times.

For each run, we measure the makespan of the application as well as the cumulative time spent in the different functions for read, processing, and writing data.
The overhead calculation for each CPU thread is the end time of the last processed task minus the total runtime of the tasks ran for this thread.
Summing those results gives the total overhead for the application.

\begin{table*}[t]
	\renewcommand{\arraystretch}{1.5}
	\caption{Parameters for the experiments}\label{table:parameters}
	\centering
	\begin{tabular}{|l|c|c|c|c|c|}
		\hline & Increment & Multi-Increment & Kmeans & Histogram & BIDS Example \\\hline
		\# of Nodes & \multicolumn{5}{c|}{2, 4, 8} \\\hline
		Dataset size [\SI{}{\giga\byte}] & 648 &\multicolumn{2}{c|}{486} & 648 & \multicolumn{1}{c|}{39} \\\hline
		Number of file & \multicolumn{4}{c|}{1000, 2500, 5000}  & \multicolumn{1}{c|}{3491} \\\hline
		\# of Iterations & 1, 5, 25 & \multicolumn{2}{c|}{1, 3, 9}                 & \multicolumn{2}{c|}{-} \\\hline
	\end{tabular}
\end{table*}

\section{Results}
\subsection{Increment}
Figure \ref{fig:increment_worker} depicts the total execution time spent in each code segment of the \textit{Increment} application.
The bars are broken down into Idle, Load, Dump, and other functions representing the overhead from the engine, the read and write time, and the aplication function, respectilvely.
The mean of each bar was calulated from 10 repetitions, with the error bar representing the standard deviation.
As expected, the compute time, from \textit{Increment}, remains the same when we vary the number of nodes.
However, the \textit{Load} and \textit{Dump} time increase with the number of nodes for both engines.
This is explained by an I/O bottleneck on the Lustre file system.

From Figure \ref{fig:increment_worker}, we also observe that Dask's overhead is higher than Spark's.
While the overhead of both engines increases with the number of nodes, Dask's overhead is significantly worse when scaling.
The difference in total execution time between the engines is explained from this overhead difference.
\begin{figure}[!h]
	\centering
	\includegraphics[clip,width=\columnwidth]{figures/stacked_increment_worker.jpg}
	\caption{Increment: varying nodes -- 5000 block resplit, 5 iterations, \SI{1}{\second} delay}
	\label{fig:increment_worker}
\end{figure}


% Block resplit
\begin{figure}[!h]
	\centering
	\includegraphics[clip,width=\columnwidth]{figures/stacked_increment_block.jpg}
	\caption{Increment: varying resplit -- 4 nodes, 5 iterations, \SI{1}{\second} delay}
	\label{fig:increment_block}
\end{figure}


%  # of iterations
\begin{figure}[!h]
	\centering
	\includegraphics[clip,width=\columnwidth]{figures/stacked_increment_itr.jpg}
	\caption{Increment: varying iterations -- 4 nodes, 5000 block resplit, \SI{1}{\second} delay}
	\label{fig:increment_itr}
\end{figure}


%  Sleep delay
% \begin{figure}[!h]
% 	\centering
% 	\includegraphics[clip,width=\columnwidth]{figures/stacked_increment_delay.jpg}
% 	\caption{Increment: varying sleep delay -- 4 nodes, 5000 block resplit, 5 iterations}
% 	\label{fig:increment_delay}
% \end{figure}

\subsection{Multi-Increment}
% TODO: experiment

\subsection{Kmeans}
% TODO: experiement
% Memory issues

\subsection{Histogram}
Figure \ref{fig:histogram_worker} depicts the execution time for the \textit{Histogram} when varying the number of nodes.
% # of nodes
\begin{figure}[!h]
	\centering
	\includegraphics[clip,width=\columnwidth]{figures/stacked_histogram_worker.jpg}
	\caption{Histogram: varying nodes -- 5000 block resplit, 5 iterations, \SI{1}{\second} delay}
	\label{fig:histogram_worker}
\end{figure}

Figure \ref{fig:histogram_worker} shows the execution time for the \textit{Histogram} when varying the number of resplit.
% Block resplit
\begin{figure}[!h]
	\centering
	\includegraphics[clip,width=\columnwidth]{figures/stacked_histogram_block.jpg}
	\caption{Histogram: varying resplit -- 4 nodes, 5 iterations, \SI{1}{\second} delay}
	\label{fig:histogram_block}
\end{figure}

\subsection{BIDS Example}
Figure \ref{fig:bids} shows the total execution time for the \textit{BIDS App example} application when varying the number of nodes.
We observe that Dask has significantly lower execution time than Spark; between 10\% to 15\%.
The main difference comes from a increased amount of stagger tasks for Spark, thus increasing the idle time of the application.
Moreover, a shorther cluster deployement time for Dask reinforce the difference in idle time.
As with other experiments, the I/O from Lustre affects the scaling of both engines as we increase the number of nodes.
\begin{figure}[!h]
	\centering
	\includegraphics[clip,width=\columnwidth]{figures/stacked_bids.jpg}
	\caption{BIDS App Example: varying nodes}
	\label{fig:bids}
\end{figure}

\section{Discussion}
% TODO

\section{Conclusion}
% TODO

\section*{Acknowledgment}
% TODO


\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,reference}
\end{document}
