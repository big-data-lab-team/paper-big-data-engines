\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{times}
\usepackage{siunitx}
\usepackage{latexsym}
\usepackage{hyperref}
\usepackage{algpseudocode}
\usepackage{algorithm}
\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}
\usepackage[caption=false]{subfig}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Comparative study of Dask and Spark for neuroimaging pipelines\\
{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
should not be used}
\thanks{Natural Sciences and Engineering Research Council}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Mathieu Dugr\'e}
\IEEEauthorblockA{\textit{Computer Science and Software Engineering} \\
\textit{Concordia University}\\
Montreal, Canada \\
mathieu.dugre@mail.concordia.ca}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
\IEEEauthorblockA{\textit{Computer Science and Software Engineering} \\
\textit{Concordia University}\\
Montreal, Canada \\
email address}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
\IEEEauthorblockA{\textit{Computer Science and Software Engineering} \\
\textit{Concordia University}\\
Montreal, Canada \\
email address}
}

\maketitle

\begin{abstract}
% Topic
As the amount of data increases and is easier to access, Big Data processing becomes
critical in neuroimaging.
% Problem
This is an issue as the currently used frameworks are specialized for neuroimaging
and not Big Data concerns. This can lead to performance decrease or limit the
research we can perform.
% Relevance
The usage of Big Data framework is beneficial to this problem. The state-of-the-art
general purpose Big Data framework Spark has its code base written in Scala, while
our laboratory mostly uses Python. This can be problematic since it is harder to port
our pipelines to the framework. Also, theoretically, it leads to performance decrease.

% Approach
We propose to use Dask since it provides native Python parallelism to pipelines while
providing support to familiar API from Python's scientific ecosystem. Unfortunately,
there are few comparisons between Spark and Dask; especially in neuroimaging.
Moreover, these studies were done when Dask was still an immature framework which
makes those studies unfair nowadays. This is our motivation to compare the latest
version of Dask with Spark.
% Methods (one sentence)
To evaluate the frameworks, we focus on their performance and their scheduler.

% Key Impact
Our study demonstrates the potential of using Dask as the framework to build
neuroimaging pipelines.
\end{abstract}

\begin{IEEEkeywords}
Big Data, Dask, Spark, performance, neuroimaging
\end{IEEEkeywords}

\section{Introduction}
% Context
With the increasing number of data available in neuroimaging,~\cite{ALFAROALMAGRO:18,
UKBioBank:18} the processing of Big Data becomes critical. Frameworks like
Nipype~\cite{Nipype:11} are usually used to create neuroimaging pipelines however it
is worth considering the use of general purpose Big Data
frameworks~\cite{Hayot-Sasson:17}. In that research, Spark~\cite{Spark:16} was a
natural choice as it is the state-of-the-art framework for Big Data. Since our
laboratory mostly works with Python, we want to know if Dask~\cite{Dask:15}, a Python
written framework, could bring better or similar performances while facilitating
usage.

%Similarities
Spark and Dask offer in-memory computing, data locality, and lazy evaluation; which
is common for Big Data framework. Both their scheduler operates dynamically. This is
good when the runtimes are not known ahead of time~\cite{Dask:15}. Over these
similarities, the frameworks are quite different.

% Spark
On the one hand, we have Spark which provides a high-level API. This allows the
scheduler to perform more optimizations which makes it well suited for neuroimaging
analysis that often requires the usage of a pipeline with multiple steps. Though,
Spark's code base is in Scala which theoretically can lead to slow down in execution
due to a required serialization. Moreover, while Spark's API is flexible and allows
most implementations, it differs from the ones seen in Python's ecosystem.

% Dask
On the other hand, Dask was created with the purpose of natively parallelize Python
pipelines while keeping the syntax of familiar API from Python's scientific
ecosystem. However, Dask is still a young framework with work to be done; it's API
does not completely replicate the library it supports. While its lower-level API
allows the implementation of more complex algorithms it sacrifices a layer of
optimization.
% Related work
Previous work shows that Dask had significant overhead and was
hard to debug~\cite{Mehta:17}.
% What issues your work addresses
Dask was immature at the time and a lot of change was brought to the framework.
Therefore we think it is valuable to re-compare it with Spark.

%% Dask API
The Dask APIs we decide to use for our comparison are Dask bag, delayed and futures.
Dask bag offers an easy API to parallelize data. Dask delayed offer a lower-level API
that offers more flexibility; this is good to implement more complex tasks that do
not fit in the Dask bag framework. Dask futures is a real-time API. Both Dask bag and
delayed apply lazy evaluation to tasks while futures trigger them directly.

% Methods used (summary)
The project aims to compare the state-of-the-art general-purpose framework Spark with
the newcomer Dask. We decide to compare the performance of Spark and Dask on a custom
incrementation pipeline to simplify the effect of the algorithm on the comparison.
Then we assess the frameworks on two real-life applications: (1) histogram of the
voxels intensity in a 3d images (2) BIDS example.

% Implications of our research
The result from our project help in deciding if Dask is a good choice to build
neuroimaging pipelines.


%%%%% MATERIAL AND METHODS %%%%%%
\section{Material and Methods}

\subsection{Engines}
Spark v2.4.0 and Dask v1.1.4 was used for the experiments.

\subsubsection{PySpark RDD~\cite{spark-rdd}} Spark is a general-purpose big data
framework. It provides in-memory computing, data locality and lazy evaluation. Spark
abstract resilient distributed dataset (RDD)~\cite{RDD} making it fault-tolerant. Its
high-level API provides optimizations on the transformations made to some data. Since
Spark is written in Scala, this optimization is lessen because it requires a
serialization from Python to Java. The API offered by Spark is flexible and suit most
problem however it differ from Python ecosystem. The standalone scheduler is used for
our experiment. It has a LIFO policy.

\subsubsection{Dask Bag~\cite{dask-bag}} Dask Bag is one of the high-level API
offered by dask. Like Spark, it provides in-memory computing, data locality and lazy
evaluation however it does not have fault resilience. Fundamently it is parallized
lists. Dask Bag parallelizes computations of large collection of generic python
objects~\cite{dask-bag}. Also, it offers optimized algorithm that mimic most of the
toolz library. Our experiment use the Dask Distributed~\cite{dask-distributed}
scheduler. It has a LIFO policy. In theory it should speed-up the pipeline excution
by finishing task computations before starting new ones.

\subsubsection{Dask Delayed~\cite{dask-delayed}} Dask Delayed is a low-level API. It
does not offer the optimized algorithm as the high-level APIs however it allows the
user to parallelize custom function that do not fit in the array, dataframe or
MapReduce framework. This is perfect for more complex function which could easily be
parallelized. Like Dask Bag, it offers in-memory computing, data locality and lack
fault resilience. The Dask Distributed scheduler.

\subsubsection{Dask Futures~\cite{dask-futures}} Dask Futures is also a low-level API
from Dask. Like the other Dask APIs it benifit of in-memory computing and data
locatlity. Unlike the other APIs, Dask Futures triggers computation immediately. When
the user tries to gather data, if available it will return the result otherwise it
will block until the cimputation is done.


\subsection{Dataset}
For the incrementation and histogram experiment the BigBrain~\cite{Amunts:13} is
used. It is a three-dimensional image of a brain scanned at \SI{40}{\micro\metre}.
The data set store the intensity of each voxel and their position. This results in a
75GB image. It is stored in 125 chunks in the minc~\cite{minc} format. To perform our
experiment we convert the chunks into the nifti format. Also, using sam~\cite{sam} we
combine the chunks and split them into 30, 125 and 750 chunks of equal size; 2.5GB,
0.6GB, and 0.1GB, respectively.

\subsection{Applications}

%% Incrementation
\subsubsection{\textbf{Incrementation}}
As our first experiment to benchmark the frameworks we used a simple incrementation
pipeline (see Algorithm~\ref{alg:incrementation}). It reads chunks from the BigBrain
image, increments the intensity value of each voxel by 1 for a specified amount of
iterations and write the result to an NFS as a Nifti image. Incrementing the data
reduce the caching effect. This experiment allows us to study the behavior of the
frameworks when all input are processed independently; i.e.\ each task in the graph
only depends on the previous one (see Figure~\ref{Figure fig:tg-inc}). This mimic the
behavior of analyzing multiple independent subjects in parallel.

This experiment refers to the implementation of the Spark pipelines purposed
in Ref.~\cite{Hayot-Sasson:17}.

\begin{algorithm}[!t]
    \caption{Incrementation}\label{alg:incrementation}
    \begin{algorithmic}
    \Require{\(x\), a sleep delay in float}
    \Require{\(file\), a file containing a chunk}
    \Require{\(fs\), NFS to save image to.}
    \State{read \(chunk\) from \(file\)}
    \ForEach{\(i \in iterations\)}
        \ForEach{\(chunk \in image\)}
            \State{\(chunk\gets chunk+1\)}
            \State{sleep \(x\)}
        \EndFor
    \EndFor
    \State{save \(chunk\) to \(fs\)}
\end{algorithmic}
\end{algorithm}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.125\textwidth,
    angle=-90]{images/incrementation-task-graph.png}
    \caption{Task graph for Incrementation}\label{fig:tg-inc}
\end{figure}

\subsubsection{Histogram}


\subsection{Experiment}

\subsection{Infrastructure}
All experiments were performed on Compute Canda Cloud.



%% Material
We compare Dask (\textbf{v1.1.4}) and Spark (\textbf{v2.4.0}) based on two metrics
their scheduling and their makespan performance. All scripts used to perform our
experiments are available at
\href{https://github.com/mathdugre/paper-big-data-engines}{https://github.com/mathdugre/paper-big-data-engines}.
For our experiments, we used Compute Canada Cloud where each instance has 8 cores,
30GB RAM, 10GB bandwidth and is dedicated.

% Histogram
\subsection{Histogram}
For our second experiment, we find the frequency of the intensity of the voxel in
BigBrain and save them as a tuple in a file. In this experiment, the results depend
on all of the input results (see~\ref{Figure fig:tg-histo}). This allows us to study
the behavior of the framework when tasks are dependent. Also, this application
requires data shuffling thus inter worker communication. We decided to omit the Dask
futures API as it is less realistic for this use case.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.16\textwidth, angle=-90]{images/histogram-task-graph.png}
    \caption{Task graph for Histogram}\label{fig:tg-histo}
\end{figure}

% BIDS Example
\subsection{BIDS Example}
Our last experiment the BIDS Example algorithm to compare the frameworks.
% Specify more details and the data set used
% SECTION TO COMPLETE -- Experience not done yet


%% Performance
\subsection{Experiments}
Our experiments have for goal to see how the frameworks react to different
environments and task types.

%% Incrementation
%% Node
Firstly, we vary the number of instances in our cluster environment. We test with 1,
2, 4, and 8 instances. From this, we can evaluate the level of parallelism offered by
both frameworks.

%% Iterations
Secondly, we modify the number of iterations we perform on the increment task between
the experiment. We use 1, 10 and 100 iterations. This allows the study of the
schedulers when a different amount of task is to be performed.

%% Sleep delay
Thirdly, we change the sleep delay in the increment tasks. We fix a 1, 4 and 16 and
64 seconds of sleep time for each iteration. This allows us to test the effect of
tasks with different time lengths.

%% Chunks
Finally, we split the BigBrain in a different number of chunks. We divide it in 30,
125 and 750 chunks; around 2.5GB, 0.6GB and 0.1GB, respectively. From that, we can
evaluate the framework with tasks of different size and IO requirements.

%% Baseline
Our baseline for all experiment is 10 iterations, 4 sleep delay, 125 chunks running
on 8 instances.

%% Histogram
For the histogram experiment, we vary the number of instances, as above. We also
change the number of chunks as above.

%% BIDS Example
For the BIDS example experiment, we vary the number of instances, again as above.

% Note
Note that the code implemented might not be optimal. We hope to provide a baseline
assessment of Dask and Spark.


%%%%% RESULTS %%%%%
\section{Results}

%%% INCREMENTATION %%%
%% Instances
\subsection{Experiment 1: Number of instances}
Figure~\ref{fig:inc_worker}(a) shows that the increase in performance is not
proportional to the number of workers which differ from what we would expect; it even
decrease when we reach 8 workers. This is explained by an increase in IO overhead
correlated to the number of workers (Figure~\ref{fig:inc_worker}(b)), as more worker
access the NFS simultaneously it increases the stress put on the system which results
in bottleneck, errors and slow down.

On the one hand, Dask seems better when using a single worker. On the other hand,
Spark makespan decreases faster for a higher level of parallelism. Dask is constantly
faster than Spark which makes us think it has a better scheduler.

\begin{figure}[!t]
    \centering
    \subfloat[Incrementation makespan]{%
      \includegraphics[clip,width=\columnwidth]{images/inc_worker.png}%
    }
    
    \subfloat[Incrementation total time]{%
      \includegraphics[clip,width=\columnwidth]{images/inc_idle_worker.png}%
    }
    \caption{125 chunks, 10 iterations, 4 sec.\ sleep delay}\label{fig:inc_worker}
\end{figure}



%% Iterations
\subsection{Experiment 1: Number of iterations}
In Figure~\ref{fig:inc_itr}, Spark and the Dask bag API are equivalent in terms of
performance. The Dask delayed API seems to perform well with a low amount of task but
grow faster the Spark and Dask bag. We think this is because Dask delayed often
schedule a block on a different thread of the same worker which causes small delays
every time due to inter-thread data communication. This is good when there are fewer
tasks as is make the IO between blocks go out of sync hence lowering the IO
bottleneck however as the number of tasks increases those small delays become
significant. The Dask futures API seems to outperform all the other APIs but this is
most likely because the tasks are not interdependent thus this application benefits
from the less optimal but faster scheduling brought by Dask futures.

\begin{figure}[!t]
    \centering
    \subfloat[Incrementation makespan]{%
      \includegraphics[clip,width=\columnwidth]{images/inc_itr.png}%
    }
    
    \subfloat[Incrementation total time]{%
      \includegraphics[clip,width=\columnwidth]{images/inc_idle_itr.png}%
    }
    \caption{125 chunks, 4 sec.\ sleep delay, 8 instances}\label{fig:inc_itr}
\end{figure}


%% Sleep time
\subsection{Experiment 1: Sleep delay}
As Figure~\ref{fig:inc_sleep}(b) and Table~\ref{tb:inc-sleep} shows Spark has the
smallest overhead all around and it is also the one that reduces the most with longer
task length. In term of overhead, Dask bag trailing slightly behind Spark although is
better than Dask delayed and Dask futures. The opposite happens for the IO time.
% Not sure what to conclude.

%%%% CHANGE NUMBER
\begin{table}[!t]
    \renewcommand{\arraystretch}{1.3}
    \caption{Distribution of the execution time in seconds for our baseline: 125
    chunks, 4 sec.\ sleep delay, 10 iterations, and 8 workers}\label{tb:inc-sleep}
    \centering
    \begin{tabular}{llllll}
    \hline
                 & Read  & Compute & Write & Overhead & Total \\ \hline
    Spark        & 34465 & 5118    & 22679 & 17076    & 79338 \\
    Dask.bag     & 36863 & 5121    & 15449 & 21388    & 78821 \\
    Dask.delayed & 32769 & 5119    & 11373 & 26199    & 75461 \\
    Dask.futures & 31891 & 5120    & 11270 & 27984    & 76265 \\ \hline
    \end{tabular}
 \end{table}

\begin{figure}[!t]
    \centering
    \subfloat[Incrementation makespan]{%
      \includegraphics[clip,width=\columnwidth]{images/inc_sleep.png}%
    }
    
    \subfloat[Incrementation total time]{%
      \includegraphics[clip,width=\columnwidth]{images/inc_idle_sleep.png}%
    }
    \caption{125 chunks, 10 iterations, 8 instances}
    \label{fig:inc_sleep}
\end{figure}


% Chunks
\subsection{Experiment 1: Number of chunks}
In Figure~\ref{fig:inc_chunk} Spark is not compared for 30 chunks. This is because
Spark has a 2GB limitation in the task size it can compute. Note from
Figure~\ref{fig:inc_chunk}(b) that the compute time increase when there are more
chunks. This is expected because the sleep delay is constant throughout this
experiment and there are more compute tasks when the number of chunks increases. We
also note that in Figure~\ref{fig:inc_chunk}(b) for 30 chunks the Dask bag API has a
much lower overhead time. This is because we only calculate the idle time of the used
core. Dask bag was only using one thread per block in comparison to Dask delayed and
Dask futures which was offloading the calculations on multiple threads of the same
worker. From Figure~\ref{fig:inc_chunk} we observe that when the dataset is separated
in more (smaller) chunks it saves IO time however it increases the overhead by a
similar amount; the inverse happens when we reduce the number of chunks.

\begin{figure}[!t]
    \centering
    \subfloat[Incrementation makespan]{%
      \includegraphics[clip,width=\columnwidth]{images/inc_chunk.png}%
    }
    
    \subfloat[Incrementation total time]{%
      \includegraphics[clip,width=\columnwidth]{images/inc_idle_chunk.png}%
    }
    \caption{10 iterations, 4 sec.\ sleep delay, 8 instances}\label{fig:inc_chunk}
\end{figure}

%% Baseline gantt chart
\subsection{Experimentation 1: Baseline timeline}
From Table~\ref{tb:inc-base} and Figure~\ref{fig:inc_gantt}, the frameworks have
similar makespan however the time spend for each type of task differs significantly.
Spark tends to spend more time than other frameworks for IO however it has a much
lower overhead. Dask delayed and futures have the lowest IO time but a much higher
overhead. Dask bag stands in the middle for both IO and overhead. Compute time is
approximately the same for all frameworks.

\begin{table}[!t]
    \renewcommand{\arraystretch}{1.3}
    \caption{Distribution of the execution time in seconds for our baseline: 125
    chunks, 4 sec.\ sleep delay, 10 iterations, and 8 workers}\label{tb:inc-base}
    \centering
    \begin{tabular}{llllll}
    \hline
                    & Read  & Compute & Write & Overhead & Total \\ \hline
    Spark        & 34465 & 5118    & 22679 & 17076    & 79338 \\
    Dask.bag     & 36863 & 5121    & 15449 & 21388    & 78821 \\
    Dask.delayed & 32769 & 5119    & 11373 & 26199    & 75461 \\
    Dask.futures & 31891 & 5120    & 11270 & 27984    & 76265 \\ \hline
    \end{tabular}
 \end{table}

\begin{figure}[!t]
    \centering
    \subfloat[Spark execution timeline]{%
      \includegraphics[clip,width=\columnwidth]{images/spark_inc_baseline_gantt.png}%
    }

    \subfloat[Dask bag execution timeline]{%
      \includegraphics[clip,width=\columnwidth]{images/Dask_bag_inc_baseline_gantt.png}%
    }

    \subfloat[Dask delayed execution timeline]{%
      \includegraphics[clip,width=\columnwidth]{images/Dask_delayed_inc_baseline_gantt.png}%
    }
    
    \subfloat[Dask futures execution timeline]{%
      \includegraphics[clip,width=\columnwidth]{images/Dask_futures_inc_baseline_gantt.png}%
    }
    \caption{125 chunks, 4 sec.\ sleep delay, 8 instances}
    \label{fig:inc_gantt}
\end{figure}


%%% HISTOGRAM %%%
\subsection{Experiment 2: Number of instances}

\begin{figure}[!t]
    \centering
    \subfloat[Histogram makespan]{%
      \includegraphics[clip,width=\columnwidth]{images/histo_instance.png}%
    }
    
    \subfloat[Histogram total time]{%
      \includegraphics[clip,width=\columnwidth]{images/histo_idle_instances.png}%
    }
    \caption{8 instances}\label{fig:histo_wroker}
\end{figure}

\subsection{Experiment 2: Number of chunks}

\begin{figure}[!t]
    \centering
    \subfloat[Histogram makespan]{%
      \includegraphics[clip,width=\columnwidth]{images/histo_splits.png}%
    }
    
    \subfloat[Histogram total time]{%
      \includegraphics[clip,width=\columnwidth]{images/histo_idle_splits.png}%
    }
    \caption{10 iterations, 4 sec.\ sleep delay, 8 instances}\label{fig:histo_chunk}
\end{figure}

%%% BIDS EXAMPLE %%%
\subsection{Experiment 3: Number of instances}



%%%%% DISCUSSION %%%%%
\section{Discussion}
\subsection{IO Time}


\subsection{Serialization}
We think that serialization could have an important effect on the makespan of the
application. This could potentially slow down the application when a lot of tasks are
scheduled.

\subsection{NFS}
The NFS seems to be a source of the bottleneck. We think that exploring another file
system, like Lustre, could give us insight into the actual effect of the NFS on the
application makespan. We also think that the NFS caching could play an effect on the
task of different lengths.

\subsection{Scheduler}
The Dask scheduler seems to have some bizarre behavior. It seems like it waits to
schedule tasks. It could also be due to tasks being scheduled on other workers
thus requiring data to be sent over the network; which would explain the stall on the
worker. More work would be needed to investigate that issue.


\subsection{Caching}
We think that caching plays an effect on the results. As seen in the Gantt chart
previously, some of the read tasks are significantly shorter than others while all
chunks are of equal size.

\section*{Acknowledgment}

We would like to thanks Compute Canada Cloud for the infrastructure.

\section*{References}
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,reference}

\end{document}
