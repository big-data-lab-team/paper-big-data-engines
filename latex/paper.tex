\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{times}
\usepackage[binary-units=true]{siunitx}
\usepackage{latexsym}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=blue,filecolor=black,urlcolor=blue}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{caption}
\usepackage{subcaption}

\newcommand{\TG}[1]{\color{cyan}From Tristan: #1 \color{black}}
\newcommand{\MD}[1]{\color{magenta}From Mathieu: #1 \color{black}}
\newcommand{\VHS}[1]{\color{green}From Valerie: #1 \color{black}}

\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{A performance comparison of Dask and Spark for data-intensive neuroimaging pipelines}

\author{Mathieu Dugr\'e, Val\'erie Hayot-Sasson, Tristan Glatard\\
Department of Computer Science and Software Engineering\\
Concordia University, Montr\'eal, Qu\'ebec, Canada
}

\maketitle

\begin{abstract}
% Topic
As the amount of data increases and is easier to access, Big Data processing becomes
critical in neuroimaging.
% Problem
This is an issue as the currently used frameworks are specialized for neuroimaging
and not Big Data concerns. This can lead to performance decrease or limit the
research we can perform.
% Relevance
The usage of Big Data framework is beneficial to this problem. The state-of-the-art
general purpose Big Data framework Spark has its code base written in Scala, while
our laboratory mostly uses Python. This can be problematic since it is harder to port
our pipelines to the framework. Also, theoretically, it leads to performance decrease.

% Approach
We propose to use Dask since it provides native Python parallelism to pipelines while
providing support to familiar API from the Python's scientific ecosystem.
Unfortunately, there are few comparisons between Spark and Dask; especially in
neuroimaging. Moreover, these studies were done when Dask was still an immature
framework which makes those studies unfair nowadays. This is our motivation to
compare the latest version of Dask with Spark.
% Methods (one sentence)
To evaluate the frameworks, we focus on their performance and their scheduler.

% Key Impact
Our study demonstrates the potential of using Dask as the framework to build
neuroimaging pipelines.
\end{abstract}

\begin{IEEEkeywords}
Big Data, Dask, Spark, performance, neuroimaging
\end{IEEEkeywords}

\section{Introduction}
% Context
The recent rise in data sharing and improved data collection strategies
have brought neuroimaging to the Big Data era ~\cite{ALFAROALMAGRO:18,
UKBioBank:18}. Existing neuroimaging workflow engines, such as
Nipype~\cite{Nipype:11}, are well suited for processing the standard
compute-intensive neuroimaging pipelines, but lack incorporated Big Data
strategies (i.e. in-memory computing, data locality and lazy-evaluation) to
improve performance of the increasingly prevalent data-intensive pipelines.
As was noted in ~\cite{hayot2019performance}, in-memory computing, coupled
with data locality, can bring significant performance improvements to data
intensive neuroimaging pipelines. We extend this work by studying the
differences between two Big Data frameworks, Dask~\cite{Dask:15} and Apache
Spark~\cite{Spark:16}, for their suitability in the processing of neuroimaging
pipelines. Our goal is to test whether Spark or Dask has a clear
performance advantage to process Big neuroimaging Data. 

%Similarities
Spark and Dask both offer in-memory computing, data locality, and lazy evaluation;
which is common for Big Data frameworks. Both their schedulers operate dynamically,
which is good when task runtimes are not known ahead of time~\cite{Dask:15}. They
also provide rich, high-level programming APIs, and support a variety of
infrastructure schedulers, such as Mesos, YARN, or HPC clusters (Dask only). Over
these similarities, the frameworks have differences.

% Differences
First and foremost, Spark is written in Scala while Dask is in Python.
Given the popularity of Python in scientific communities, this arguably
gives an edge to Dask due to data serialization costs from Python to Scala.
On the other hand, Python's Global Interpreter Lock (GIL) might reduce
parallelism in some cases. This difference in programming languages also
has qualitative implications. As part of the \href{http://scipy.org}{SciPy}
ecosystem, Dask provides almost transparent parallelization of applications
manipulating Numpy arrays or Pandas data frames. On the other hand, Spark's
Java, R and Python APIs allow to easily parallelize analyses that combine
these languages, with reduced performance loss.

% Related work
Spark and Dask were both included in the evaluation reported
in~\cite{Mehta:17} that used a neuroimaging and an astronomy application
processing approximately \SI{100}{\giga\byte} of data each. In this work, Dask was
reported to have a slight performance advantage over Spark. Overall, Dask's
end-to-end time (makespan) was measured to be up to 14\% faster than Spark,
due to ``more efficient pipelining'' %(Fig 10) 
and serialization time to
Python. % (Fig. 12-a)
Dask, however, was reported to have a larger startup
time than Spark. % (Fig. 12-c)
The analysis remains at a quite high level
though, leaving most of the observed performance difference unexplained. In
comparison, our study will provide a detailed analysis of performance
differences and similarities between Spark and Dask.

The next section details the design of our experiments. We consider two
data-intensive neuroimaging applications: high-resolution imaging,
represented by the Big Brain data~\cite{Amunts:13}, and large functional
MRI studies, represented by data from the consortium for reliability and
reproducibility (CoRR~\cite{zuo2014open}). We test application pipelines
involving different patterns (map-only, map-reduce), and different types of
implementations (plain Python, command-line,
containerized). We evaluate performance on a dedicated cluster
representative of the ones used in today's data-intensive neuroimaging
studies, using the main Dask and Spark APIs. The following sections present
our results, discussion and conclusions.



%%%%% MATERIAL AND METHODS %%%%%%
\section{Material and Methods}

\subsection{Engines}

\subsubsection{Apache Spark} Apache Spark is a general-purpose Big Data engine.
While its core is written in Scala, Spark provides additional APIs in Java, R
and Python (PySpark). Its main abstraction, the Resilient Distributed Dataset
(RDD)~\cite{RDD}, is a fault-tolerant, parallel collection of data elements. This
data structure is the basis of Spark's other data structures, namely, DataFrames
and Datasets. Datasets are similar to RDDs but additionally use Spark SQL's
optimized execution to further improve performance. DataFrames, used to process
tabular data, Datasets where the data is organized into named-columns. While 
the DataFrame API exists in all the available language APIs, Datasets are
limited to Scala and Java.

As data transfers in BigData workflows are an important source of performance
bottlenecks, Spark incorporated
data locality, and additionally, introduced in-memory computing to the Big Data 
frameworks. Data locality, introduced by MapReduce~\cite{mapreduce}, schedules
tasks as close as possible to where the data is storage. In-memory computing ensures
data is maintained in memory whenever possible, as writting large amounts data to
slower storage devices may be costly. To reduce any unecessary communication
and computation, Spark also included lazy evaluation, which builds the entire
task graph prior to execution to determine what needs to be computed.


Spark is compatible with three different schedulers: Spark
standalone, YARN~\cite{vavilapalli2013apache} and Mesos~\cite{hindman2011mesos}. 
The Spark standalone is a simple default scheduler built into Spark. YARN is a
more complex scheduler primarily designed to schedule Hadoop-based workflows. 
Mesos, on the other hand, is not limited to Hadoop and can be used to schedule
a variety of different workflows. As scientific researchers would likely be
executing their workflows in HPC environments with neither YARN or Mesos
installed, we limit our focus to Spark's Standalone scheduler.


The Spark Standalone scheduler is composed of three main processes: the
\emph{master}, the \emph{workers} (\emph{slaves}, in Spark) and the
\emph{driver}. The \emph{master} coordinates resources provisioned by
\emph{worker} processes on the cluster. The application is submitted to the
\emph{driver} that in turn requests workers to the master and dispatches tasks
to them. A job is divided into stages to be excuted in a different process onto
the workers. Each stage's operation is represented as a high-level task in the computation
graph. The Spark standalone scheduler uses a FIFO (First-In-First-Out) job
scheduling policy. The Spark standalone scheduler has two
execution modes: client mode, where the driver runs in a dedicated process outside
of the Spark cluster, and cluster mode, where the driver runs in a worker. We used
client mode as cluster mode is not available in PySpark. We used Apache Spark v2.4.0.


Python is commonly selected as the programming language of choice in scientific
communities, especially for our use case of neuroscience, where numerous specialized
Python libraries exist to study the data. While serialization of 
Python to Java may lead to significant overheads, we chose to focus on Spark 
Python API due to its suitability for neuroimaging research. 


\subsubsection{Dask} Dask is a Big Data engine that is becoming increasingly popular
in the scientific Python ecosystem. Dask has five main data structure APIs:
Array, Bag, DataFrame, Delayed, and Futures. All APIs were used in our experiments,
with the exception of Dask Dataframes, which is intended for use with tabular data.
Like Spark, all Dask APIs provide
in-memory computing, data locality, lazy evaluation, and fault tolerance by mean of
lineage. Dask's core is purely in Python and bypasses most of the
\href{https://docs.python.org/3/glossary.html#term-gil}{GIL} to provide parallelism.
We used Dask due to the common use of the scientific Python ecosystem in
neuroscience\VHS{If you add Dask Arrays to your experiments here, i'd mention that their
APIs might be particularly well suited for the processing of high resolution neuroimaging data}. Dask represents application tasks as Dask graphs, produced by a
user-facing API and executed by a scheduler. In Dask, operations generate multiple
tiny tasks in the computation graph allowing an easier representation of complex
algorithms. We used the \href{https://distributed.dask.org/en/latest/index.html}{Dask
Distributed} scheduler, although \href{https://github.com/dask/dask-yarn}{Dask-Yarn}
and \href{https://github.com/mrocklin/dask-mesos}{Dask-Mesos} is also available. In
the Dask Distributed scheduler, a process called \textit{dask-scheduler}
administrates the resources provided by \textit{workers} in the cluster. The
scheduler receives jobs from clients and assigns tasks to available workers. Like
Spark scheduler, Dask Distributed also use a LIFO policy, it is meant to reduce the
memory footprint of the application by finishing the computation of a branch in the
Dask graph before starting new ones. We used Dask v1.1.4. 

In Dask, a \href{https://docs.dask.org/en/latest/bag.html}{Bag} is a parallel
collection of Python objects, similar to Spark's RDD. It use multi-process to enable
parallelism. It offers a programming abstraction similar to the
\href{https://toolz.readthedocs.io/en/latest/}{PyToolz library}. An
\href{https://docs.dask.org/en/latest/array.html}{Array} is used for the processing
of large arrays. It provides a distributed clone of the popular NumPy library. A
\href{https://docs.dask.org/en/latest/dataframe.html}{Dataframe} is a parallel
composition of
\href{http://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html}{Pandas
Dataframes} used to process a large amount of tabular data.
\href{https://docs.dask.org/en/latest/delayed.html}{Delayed} supports arbitrary tasks
that do not fit in the Array, DataFrame or Bag APIs. Finally,
\href{https://docs.dask.org/en/latest/futures.html}{Futures} are similar to Delayed
as they support arbitrary tasks, but they operate in real-time rather than lazily.

\subsection{Infrastructure}

 We used Compute Canada's
 \href{https://docs.computecanada.ca/wiki/Cloud_resources}{Arbutus Cloud} operated by
 the \href{https://www.westgrid.ca}{WestGrid} regional organization at the University
 of Victoria, and running OpenStack \TG{version?}. We used c8-30gb-186 cloud
 instances with 8 VCPUs, an Intel Xeon Gold 6130 processor, \SI{30}{\giga\byte} of
 RAM at \SI{2666}{\mega\hertz}, \SI{20}{\giga\byte} of mounted storage, and a base
 image running CentOS 7.5.1804 with Linux kernel version
 3.10.0\-862.11.6.el7.x86\_64. Instances are connected by a
 \SI{10}{\giga\bit/\second} Ethernet network.
 
 Cloud instances hosted a single Dask or Spark worker, configured to use 8 CPUs. We
 used Dask's default configuration that uses all the available memory on the
 instance. Its default heuristic is to: target a 60\% memory load, spill to disk at
 70\%, pause the worker at 80\%, and terminate the worker at 95\%. We configured
 Spark to use 1 executor per worker and \SI{25}{\giga\byte} of memory per executor, to leave 5~GB
 for off-heap. We configured the Spark driver to use 25~GB of memory, and used the
 default configuration for the master. We used the default confirguration for worker
 memory management: at 60\% it spill data to disk, and 50\% of that amount is
 reserved for storage and is immuned from eviction.
 
 One cloud instance did not host any worker, and had a \SI{2}{\tera\byte} disk volume
 shared with the other instances using the Network File System (NFS) v4.
 This instance was also used for the Spark driver and master, and for the
 Dask scheduler, and for job monitoring with the Spark and Dask user
 interfaces. For both Spark and Dask, spilled data is evicted to the NFS.

\subsection{Dataset}

We used BigBrain~\cite{Amunts:13}, a three-dimensional image of a human
brain with voxel intensities ranging from 0 to 65,535. The original data is
stored in 125 blocks in the MINC~\cite{minc} HDF5-based format, available
at \url{ftp://bigbrain.loris.ca/BigBrainRelease.2015/3D_Blocks/40um} at the
resolution of \SI{40}{\micro\metre}. We converted the blocks into the
\href{https://nifti.nimh.nih.gov/nifti-1}{NifTI} format, a popular format
in neuroimaging. We left the NifTI blocks uncompressed, resulting in 
a total data size of \SI{81}{\giga\byte}. 
To evaluate the effect of block size, we resplit these blocks into 30, 125 and 750 blocks of 
\SI{2.7}{\giga\byte}, \SI{0.648}{\giga\byte}, and
\SI{0.108}{\giga\byte}, using the sam~\cite{sam} library.

We also used the dataset provided by the Consortium for Reliability and
Reproducibility (\href{http://fcon_1000.projects.nitrc.org/indi/CoRR/html/}{CoRR}) as
available on \href{http://datasets.datalad.org/?dir=/corr/RawDataBIDS}{DataLad}. The
entire dataset is \SI{408.4}{\giga\byte}, containing anatomical, diffusion and
functional images of 1,397 subjects acquired in 29 sites. We used all 3491 anatomical
images, representing \SI{39}{\giga\byte} overall (\SI{11.17}{\mega\byte} by image on
average).


\subsection{Applications}

We used three neuroimaging applications to evaluate the engines in different
conditions. The first two ones, incrementation and histogram, are simple synthetic
benchmarks representing basic map-only and map-reduce applications. The third one is
a realistic application representative of popular BIDS
applications~\cite{gorgolewski2017bids}. All script used for our experiment is availble
GitHub at \url{https://github.com/big-data-lab-team/paper-big-data-engines}

%% Incrementation
\subsubsection{Incrementation}
We used an adaptation of the simple image incrementation pipeline used
in~\cite{hayot2019performance} (see Algorithm~\ref{alg:incrementation}).
The application reads blocks of the BigBrain image from the shared file
system, increments the intensity value of each voxel by 1 to avoid caching
effects, sleeps for a configurable amount of time to emulate a more complex
processing, repeats this process for a specified amount of iterations, and
finally writes the result as a NifTI image back to the shared file system.
This application allows us to study the behavior of the engines when all
inputs are processed independently, in a map-only way (see
Figure~\ref{fig:tg-inc}). This mimics the behavior of analyzing multiple
independent subjects in parallel.

\begin{algorithm}[!b]
    \caption{Incrementation (adapted from~\cite{hayot2019performance})}\label{alg:incrementation}
    \begin{algorithmic}
    \Require{\(x\), a sleep delay in float}
    \Require{\(file\), a file containing a block}
    \Require{\(fs\), NFS to write image to.}
    \State{Read \(block\) from \(file\)}
    \ForEach{\(i \in iterations\)}
        \ForEach{\(block \in image\)}
            \State{\(block\gets block+1\)}
            \State{Sleep \(x\)}
        \EndFor
    \EndFor
    \State{Write \(block\) to \(fs\)}
\end{algorithmic}
\end{algorithm}

\begin{figure}[!b]
    \centering
    \includegraphics[width=0.125\textwidth,
    angle=-90]{images/incrementation-task-graph.png}
    \caption{Task graph for Incrementation with 5 iterations and 3 BigBrain blocks.
    Circles represent the incrementation and sleep function while rectangles
    represent stages of the image blocks.}\label{fig:tg-inc}
\end{figure}

\subsubsection{Histogram}

 Our second application calculates the histogram of the BigBrain image (see
 Algorithm~\ref{alg:histogram}). It reads the image blocks from the shared file
 system, calculates intensity frequencies, aggregates the frequencies across the
 blocks, and finally writes the resulting histogram to the shared file system as a
 single \SI{766}{\kilo\byte} file. This application implements a typical map-reduce
 pattern, where the final result is obtained from all the input blocks (see
 Fig.~\ref{fig:tg-histo}). This application requires data shuffling thus inter-worker
 communication. The total amount of shuffled data is however limited to
 \SI{2.62}{\mega\byte} per blocks as it only consists of image histograms.

\begin{algorithm}[!t]
    \caption{Histogram}\label{alg:histogram}
    \begin{algorithmic}
    \Require{\(files\), files containing BigBrain}
    \Require{\(fs\), NFS to save image to.}
    \ForEach{\(file \in files\)}
        \State{Read \(block\) from \(file\)}
        \State{{Calculate \(frequency\) of \(blocks\)}}
    \EndFor
    
    \State{\(histogram\gets\)Aggregate \(frequency\) of each \(file\)}

    \State{Write \(histogram\) to \(fs\)}
    \end{algorithmic}
\end{algorithm}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.16\textwidth, angle=-90]{images/histogram-task-graph.png}
    \caption{Task graph for Histogram with 3 BigBrain blocks. Circles represent the
    functions while rectangles represent stages of the image blocks.
    }\label{fig:tg-histo}
\end{figure}

\subsubsection{BIDS app example}

We used the \href{https://github.com/BIDS-Apps/example}{BIDS app example} application
available on GitHub. This application operates in a map-reduce way. The map phase,
also called participant analysis, extracts the brain tissues of a subject's 3D MRI
using the popular FMRIB Software Library
(\href{https://fsl.fmrib.ox.ac.uk/fsl/fslwiki}{FSL}), and writes the resulting image
(\SI{2.47}{\mega\byte} on average) to the shared file system. The reduce phase, also
called group analysis, computes the volume of each brain and returns the average
volume, shuffling a total of \SI{8.6}{\giga\byte} image data.

While Incrementation and Histogram were implemented directly in Python, BIDS app
example requires an external library and is distributed as a Docker container image
(bids/base\_fsl on DockerHub). We converted the Docker image to a Singularity image,
given its popularity for HPC and reproducibility, using
\href{https://hub.docker.com/r/singularityware/docker2singularity/tags/}{docker2singularity}.
The image was preloaded on the nfs and singularity version 3.2.1\-1.el7 was installed
on all instances. Due to a bug in the conversion from Docker to Singularity, we had to
export the path for fsl.

\subsection{Experiments}

We varied four parameters in our experiments, as shown in
Table~\ref{tab:param}. We varied (1) the number of workers to assess the
scalability of the engines, (2) the number of BigBrain blocks in
Incrementation and Histogram to measure the effect of different IO patterns
and parallelization degrees, and (3) the number of iterations and sleep
delay in Incrementation to evaluate the effect of job length.
It should be noted that increasing the number of blocks or iterations also
increases the total compute time of the application for a given sleep
delay. To avoid any potential external bias such as background load on the
network, we ran the experiments in a randomized order and cleared the page
cache of each worker before each execution.

For each run, we measured the application makespan as well as the cumulative 
data read, compute, data write, and engine overhead across all application tasks. 
We measured the overhead by \TG{\ldots, maybe add a figure}. \MD{Done but might need
adjustment depending on how the overhead is calculate (idle vs wasted)}

\TG{explain that number of iterations may impact futures.}

% The baseline for our experiments is: 8 workers, 125 blocks, 10 iterations and 4
% seconds sleep delay.

\begin{table}[!t]
    \renewcommand{\arraystretch}{1.3}
    \caption{Parameters for the experiments}\label{tab:param}
    \centering
    \begin{tabular*}{\columnwidth}{llll}
    \hline
                        & Incrementation & Histogram             & BIDS Example          \\ \hline
    \# of worker        & 1, 2, 4, 8     & 1, 2, 4, 8            & 1, 2, 4, 8            \\
    \# of blocks        & 30, 125, 750   & 30, 125, 750          & \multicolumn{1}{c}{-} \\
    \# of iterations    & 1, 10, 100     & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} \\
    Sleep delay {[}\SI{}{\second}{]} & 1, 4, 16, 64   & \multicolumn{1}{c}{-} & \multicolumn{1}{c}{-} \\ \hline
    \end{tabular*}
    \end{table}






%%%%% RESULTS %%%%%
\section{Results}

%%% INCREMENTATION %%%
%% worker
\subsection{Incrementation: Number of workers}
Figure~\ref{fig:inc_ms_worker} shows the makespan of the Incrementation application
for different numbers of workers and for different engines. The bars show the average
makespan over 3 repetitions while the error bars are the standard deviation. Overall,
there is no substantial makespan difference between the engines. Dask seems to have a
slight advantage over Spark, \SI{83.61}{\second} on average,
with Delayed and Futures being slightly better than Bags.

For all engines, the makespan is far from decreasing linearly with the
number of workers. Note that there are 8 threads per worker. The makespan
even increases between 4 and 8 workers. This is due to the high impact of
data transfers and engine overhead on the application. 

Figure~\ref{fig:inc_tt_worker} shows the total execution time of the
Incrementation application, broken down into data transfer (read and
write), compute (sleep), and overhead time. As expected, the computing time
stays similar when the number of workers increases. However, the data
transfer time and overhead increase proportionally to the number of
workers \TG{check if indeed proprotional, add regression slopes}.

On Figure~\ref{fig:inc_tt_worker}, we also note that Spark's overhead is
slightly lower than Dask's in particular as the number of workers increase.
Within Dask, Delayed and Futures have a higher overhead than Bags. However,
overhead differences are compensated by an increase in data transfer time,
as a reduced overhead increases the concurrency between data transfers. 

\begin{figure}[!b]
    \centering
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/inc_worker.png}%
        \caption{Incrementation makespan}\label{fig:inc_ms_worker}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/inc_idle_worker.png}%
        \caption{Incrementation total time}\label{fig:inc_tt_worker}
    \end{subfigure}
    \caption{125 blocks, 10 iterations, \SI{4}{\second} delay, 8
    threads/worker}\label{fig:inc_worker}
\end{figure}

% blocks
\subsection{Incrementation: Number of blocks}

Figure~\ref{fig:inc_ms_block} shows the Incrementation makespan when varying the
number of image blocks for constant BigBrain image size. We were not able to run
Spark for 30 blocks due to its \SI{2}{\giga\byte} limitation in the task size it can
compute. Once again, we do not observe any significant difference among the engines.
For all engines, makespan variability increases with the number of blocks, however,
engines scale very well in general.


In Figure~\ref{fig:inc_tt_block}, the total execution time of each function is shown.
For 30 blocks the Dask Bag API has a much lower overhead time. This is because Bag
was only using one thread per block in comparison to Delayed and Futures that were
offloading block calculations on multiple threads of the same worker. Once again,
the data transfer time reduces with more blocks however the overhead time increases
by a similar amount. This is not observed for 30 blocks as the workers are not used
at full capacity, i.e., some threads are idle. Finally, the variability of the
overhead increases with the number of blocks, which explains the makespan variability
mentioned previously.

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/inc_block.png}%
        \caption{Incrementation makespan}\label{fig:inc_ms_block}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/inc_idle_block.png}%
        \caption{Incrementation total time}\label{fig:inc_tt_block}
    \end{subfigure}
    \caption{10 iterations, \SI{4}{\second} delay, 8 workers, 8
    threads/worker}\label{fig:inc_block}
\end{figure}

%% Iterations
\subsection{Incrementation: Number of iterations}
Figure~\ref{fig:inc_ms_itr} shows the makespan of the application while
varying the number of iterations. Overall,  Spark and Dask APIs are once
again equivalent, although Dask Delayed and Futures are slightly faster
than Bags and, Spark for 1 and 10 few iterations, and Futures are faster than
Delayed, Bags and Spark for 100 iterations. Differences remain minor
though.

In Figure~\ref{fig:inc_tt_itr}, the total execution time breakdown is
shown. We observe the good scalability of all the engines with the number of
iterations.
% TODO

% We think this is because Dask Delayed often
% schedule a block on a different thread of the same worker which causes small delays
% every time due to inter-thread data communication. This is good when there are fewer
% tasks as is make the IO between blocks go out of sync hence lowering the IO
% bottleneck however as the number of tasks increases those small delays become
% significant.

%  The Dask Futures API seems to outperform all the other APIs but this is
% most likely because the tasks are not interdependent thus this application benefits
% from the less optimal but faster scheduling brought by Dask Futures.

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/inc_itr.png}%
        \caption{Incrementation makespan}\label{fig:inc_ms_itr}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/inc_idle_itr.png}%
        \caption{Incrementation total time}\label{fig:inc_tt_itr}
    \end{subfigure}
    \caption{125 blocks, \SI{4}{\second} delay, 8 workers, 8 threads/worker}
\end{figure}

%% Sleep time
\subsection{Incrementation: Sleep delay}
Figure~\ref{fig:inc_ms_sleep} shows the makespan of the Incrementation application
for different sleep delays. Overall, all engines again perform the same and scale
well with task duration. Spark is initially slower than the Dask APIs, however, it is
faster when increasing the sleep delay. Also, within the Dask APIs, Dask Bag is
slower than the other two but it is not considerable.

Figure~\ref{fig:inc_tt_sleep} shows the total execution time breakdown. On
one hand, Spark has the smallest overhead. As previously observed,
variations in overhead time are almost exactly compensated by variations in data
transfer time.

% \begin{table}[!t]
%     \caption{Function time for the sleep experiment}
%     \begin{subtable}[b]{\columnwidth}
%         \renewcommand{\arraystretch}{1.3}
%         \caption{Overhead time in second}\label{tab:inc_sleep_overhead}
%         \centering
%         \begin{tabular}{lllll}
%         \hline
%                      & 1 sec. & 4 sec. & 16 sec. & 64 sec. \\ \hline
%         Spark        & 14992  & 17076  & 16069   & 9769    \\
%         Dask.Bag     & 20690  & 21388  & 17759   & 18720   \\
%         Dask.Delayed & 27481  & 26199  & 22080   & 24624   \\
%         Dask.Futures & 27069  & 27984  & 23538   & 24762   \\ \hline
%         \end{tabular}
%     \end{subtable}
%     \vskip 0.2cm
%     \begin{subtable}[b]{\columnwidth}
%         \renewcommand{\arraystretch}{1.3}
%         \caption{IO time in second}\label{tab:inc_sleep_io}
%         \centering
%         \begin{tabular}{lllll}
%         \hline
%                      & 1 sec. & 4 sec. & 16 sec. & 64 sec. \\ \hline
%         Spark        & 63622  & 57144  & 51070   & 57078   \\
%         Dask.Bag     & 54630  & 52312  & 53743   & 53612   \\
%         Dask.Delayed & 45239  & 44142  & 45133   & 46198   \\
%         Dask.Futures & 46537  & 43161  & 44749   & 45846   \\ \hline
%         \end{tabular}
%     \end{subtable}
%     \vspace{-3mm}
%  \end{table}

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/inc_sleep.png}%
        \caption{Incrementation makespan}\label{fig:inc_ms_sleep}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/inc_idle_sleep.png}%
        \caption{Incrementation total time}\label{fig:inc_tt_sleep}
    \end{subfigure}
    \caption{125 blocks, 10 iterations, 8 workers, 8 threads/worker}
\end{figure}

%% Baseline gantt chart
\subsection{Incrementation: Gantt chart}

Figure~\ref{fig:inc_gantt} shows the Gantt chart obtained for each engine and API.
Gantt charts are structured in batches of up to 64 read-compute-write concurrent
sequences. File reads in the first batch are much longer than in the following ones:
this is due to the high synchronization of data transfers that leads to a high
saturation of the shared file system. We also note that overhead, represented in
white, is concentrated around the data transfer tasks \TG{find out why}, and the
computing tasks that run concurrently with data transfers. 
% The frameworks have similar makespan however the time spend for each type of
% task differs considerably (see Table~\ref{tab:inc_base}). Spark tends to spend more
% time than other frameworks for IO however it has a much lower overhead. Dask Delayed
% and Futures have the lowest IO time but a much higher overhead. Dask Bag stands in
% the middle for both IO and overhead. Compute time is approximately the same for all
% frameworks. Apart from the first IO section, there is a large amount of ovehead
% everytime time IO is performed. This is true for all frameworks.

\begin{figure*}[!htb]
    \centering
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/spark_inc_gantt.png}
        \caption{Spark}\label{fig:inc_spark_gantt}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/bag_inc_gantt.png}%
        \caption{Dask Bag}\label{fig:inc_dask_bag_gantt}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/delayed_inc_gantt.png}%
        \caption{Dask Delayed}\label{fig:inc_dask_delayed_gantt}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/futures_inc_gantt.png}%
        \caption{Dask Futures}\label{fig:inc_dask_futures_gantt}
    \end{subfigure}
    \caption{Incrementation: Gantt chart with 125 blocks, 10 iterations, \SI{4}{\second}
delay, 8 workers, 8 threads/worker}\label{fig:inc_gantt}
\end{figure*}

% \begin{table}[!b]
%     \renewcommand{\arraystretch}{1.3}
%     \caption{Distribution of the execution time in second: 125 blocks, 4 sec.\ sleep
%     delay, 10 iterations, and 8 workers}\label{tab:inc_base}
%     \centering
%     \begin{tabular*}{\columnwidth}{llllll}
%     \hline
%                  & Read  & Compute & Write & Overhead & Total \\ \hline
%     Spark        & 34465 & 5118    & 22679 & 17076    & 79338 \\
%     Dask Bag     & 36863 & 5121    & 15449 & 21388    & 78821 \\
%     Dask Delayed & 32769 & 5119    & 11373 & 26199    & 75461 \\
%     Dask Futures & 31891 & 5120    & 11270 & 27984    & 76265 \\ \hline
%     \end{tabular*}
%  \end{table}

%%% HISTOGRAM %%%
\subsection{Histogram: Number of workers}
Figure~\ref{fig:histo_ms_worker} shows the makespan of the application for various
amounts of workers. Spark is tremendously faster than Dask APIs. The difference
narrows as the number of workers increases. Between Dask APIs there is no substantial
difference, however, Bag is slightly faster on average.

From Figure~\ref{fig:histo_tt_worker}, the total time spent in each function is shown.
The computation time is significantly larger in Dask than Spark. This is potentially
due to Python's GIL preventing Dask to parallelize the computation on multiple threads.
Overall, the IO and overhead are comparable for all engines.

On Figure~\ref{fig:histo_tt_worker}, Dask engines have similar total execution time
when their number of workers varies, except for 8 workers where it increases slightly.
On the other hand, Spark total execution time keeps increasing; especially at 8
workers. This is because Dask engines benefit more from additional workers.

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/histo_worker.png}%
        \caption{Histogram makespan}\label{fig:histo_ms_worker}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/histo_idle_worker.png}%
        \caption{Histogram total time}\label{fig:histo_tt_worker}
    \end{subfigure}
    \caption{125 blocks, 8 threads/worker}
\end{figure}

\subsection{Histogram: Number of blocks}
Figure~\ref{fig:histo_ms_block} shows the makespan for different block size.
Again, Spark is significantly faster then Dask engines and Bag is slightly faster
than Delayed. Overall, the engines do not react to change in block size.

In figure~\ref{fig:histo_tt_block} the total time for each function is shown.
Note that at 30 blocks the worker's resources are not fully used since the workers
can process up to 64 tasks in parallel. Moreover, the overhead for 30 blocks is
erroneous due to a lower amount of block than available threads. This leads to thread
based engines to have higher overhead. Given that, 

Spark has a much lower compute time which makes it significantly faster than Dask
engines. Overall, for all engines, lower block size results in lower IO time but
increases the overhead time. Again, this is due to the overhead desynchronizing task
thus reducing the NFS bottleneck.
%TODO

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/histo_block.png}%
        \caption{Histogram makespan}\label{fig:histo_ms_block}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/histo_idle_block.png}%
        \caption{Histogram total time}\label{fig:histo_tt_block}
    \end{subfigure}
    \caption{8 workers, 8 threads/worker}
\end{figure}
\TG{We decided to omit Dask Futures as it does not have any value over Dask
Delayed in this application.}

\subsection{Histogram: Gantt chart}
Figure~\ref{fig:histo_gantt} shows the Gantt chart of the baseline Histogram
experiment. Spark overhead is distributed through all the execution while Bag
overhead happens mostly when a thread read subsequent blocks and Delayed overhead is
concentrated after reading all the blocks.

The reduce is done differently on the engines. While Spark and Delayed compute all
the reduce tasks on one process (thread) Spark computes it on the driver process. On
the other hand, Bag computes the reduce tasks on different workers and sends the
information to the driver process afterward.

\begin{figure}[!htb]
    \centering
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/spark_histo_gantt.png}
        \caption{Spark}\label{fig:histo_spark_gantt}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/bag_histo_gantt.png}%
        \caption{Dask Bag}\label{fig:histo_dask_bag_gantt}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/delayed_histo_gantt.png}%
        \caption{Dask Delayed}\label{fig:histo_dask_delayed_gantt}
    \end{subfigure}
    \caption{Histogram: Gantt chart with 125 blocks, 10 iterations, \SI{4}{\second}
delay, 8 workers, 8 threads/worker}\label{fig:histo_gantt}
\end{figure}

%%% HISTOGRAM (NumPy) %%%
\subsection{Histogram (NumPy): Number of workers}
Figure~\ref{fig:histo_np_ms_worker} shows the makespan for various amount of workers.
Overall, the makespan for all engines does not change considerably with an increase
in the number of workers. This is because the engines do not scale well.

On figure~\ref{fig:histo_np_ms_worker}, there is no substantial difference between
 all engines, although, Delayed is slightly faster.

From Figure~\ref{fig:histo_np_tt_worker}, the total execution time spent in each
function is shown. The overhead and read time increase proportionally to the number
of workers thus increasing the total time. This is because (1) most workers read at
the same time due to a low computation-IO ratio and (2) \MD{Determine why the
overhead is so large compared to the pure Python implementation}

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/histo_np_worker.png}%
        \caption{Histogram (NumPy) makespan}\label{fig:histo_np_ms_worker}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/histo_idle_np_worker.png}%
        \caption{Histogram (NumPy) total time}\label{fig:histo_np_tt_worker}
    \end{subfigure}
    \caption{125 blocks, 8 threads/worker}
\end{figure}

\subsection{Histogram (NumPy): Number of blocks}
Figure~\ref{fig:histo_np_ms_block} shows the makespan of the application for
different block size. Note that the workers can process up to 64 tasks in parallel
thus they are not used at full capacity with 30 blocks. Overall, there is no
substantial difference for all engines and block size.

In figure~\ref{fig:histo_np_tt_block}, shows the total time spent in each function.
Again, the overhead for 30 blocks is erroneous due to a lower amount of task than
available threads which result in higher overhead for thread-based engines.
Considering the overhead error and that at 30 blocks the workers only use half their
core, the total time is substantially the same for all block size. This is due to
lower block sizes having a lower IO time, however, it is balanced by the resulting
increase in overhead.

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/histo_np_block.png}%
        \caption{Histogram (NumPy) makespan}\label{fig:histo_np_ms_block}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/histo_idle_np_block.png}%
        \caption{Histogram (NumPy) total time}\label{fig:histo_np_tt_block}
    \end{subfigure}
    \caption{8 workers, 8 threads/worker}
\end{figure}

\subsection{Histogram (NumPy): Gantt chart}
Figure~\ref{fig:histo_np_gantt} shows the Gantt chart of the baseline Histogram
(NumPy) experiment. The overhead for Spark and Bag is mostly located between a read
and compute task or compute and read task. On the other hand, Delayed overhead is
dispersed between all types of tasks. This causes Delayed to overlap more compute and
read tasks among the different workers.

In figure~\ref{fig:histo_np_gantt}, The read tasks of all engines are similar initially, however, when computation starts
the subsequent read tasks are much shorter. This is because read tasks for Delayed
become desynchronized hence it significantly reduces the IO bottleneck.



\begin{figure}[!htb]
    \centering
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/spark_histo_np_gantt.png}
        \caption{Spark}\label{fig:histo_np_spark_gantt}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/bag_histo_np_gantt.png}%
        \caption{Dask Bag}\label{fig:histo_np_dask_bag_gantt}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/delayed_histo_np_gantt.png}%
        \caption{Dask Delayed}\label{fig:histo_np_dask_delayed_gantt}
    \end{subfigure}
    \caption{Histogram (NumPy): Gantt chart with 125 blocks, 10 iterations, \SI{4}{\second}
delay, 8 workers, 8 threads/worker}\label{fig:histo_np_gantt}
\end{figure}

%%% BIDS EXAMPLE %%%
\subsection{BidsApp example: Number of workers}
Figure~\ref{fig:bids_ms_worker} shows the makespan of the application while varying
the amount of worker. Overall, there is no substantial difference between the
engines. The makespan scales well with the increase in workers however the scaling
reduces considerably when reaching 8 workers. This is due to the IO requirement
increasing with the number of workers --- more tasks are read concurrently and
the application becomes more data-intensive.

In Figure~\ref{fig:bids_tt_worker}, the total execution time of each function is
shown. Overall, there is no significant difference between the engines. Each engine
has a similar total time independently of its number of workers. This is because of
the, almost, linearly scale of the engines. The overhead increases slowly as the
number of workers increases. This is due to more communication between the scheduler
and workers as well as inter-worker communication.

\begin{figure}[!t]
    \centering
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/bids_worker.png}%
        \caption{BIDS App example makespan}\label{fig:bids_ms_worker}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/bids_idle_worker.png}%
        \caption{BIDS App example total time}\label{fig:bids_tt_worker}
    \end{subfigure}
    \caption{Variation of the amount of worker, 8 threads/worker}
\end{figure}


%% Baseline gantt chart
\subsection{BidsApp example: Gantt chart}
Figure~\ref{fig:bids_gantt} shows the Gantt charts obtained for each engine and API.
The Gantt charts are structured in two main parts: participant analysis (orange) and
group analysis (blue). The participant analysis tasks differ greatly in length. This
is due to the unequal amount of sessions per subject to process. The group analysis
is similar for all engines and APIs. Overall, most of the overhead encountered
results from the transition between the two analysis. This is because the group
analysis requires the results of the participant analysis to start.

\begin{figure*}[!htb]
    \centering
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/spark_bids_gantt.png}
        \caption{Spark}\label{fig:bids_spark_gantt}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/bag_bids_gantt.png}%
        \caption{Dask Bag}\label{fig:bids_dask_bag_gantt}
    \end{subfigure}
    \\
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/delayed_bids_gantt.png}%
        \caption{Dask Delayed}\label{fig:bids_dask_delayed_gantt}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{\columnwidth}
        \includegraphics[clip,width=\columnwidth]{images/futures_bids_gantt.png}%
        \caption{Dask Futures}\label{fig:bids_dask_futures_gantt}
    \end{subfigure}
    \caption{BIDS App example: Gantt chart with 8 workers}\label{fig:bids_gantt}
\end{figure*}



%%%%% DISCUSSION %%%%%
\section{Discussion}

\TG{Talk about engine overhead: (1) where is it coming from? (2) 
explain the impact on data transfers with a graph.}

\TG{In the dicussion, get back to it and mention that (1) we don't
always observe that, (2) for data-intensive applications, overheads don't really matter
as reducing them increases data trasnfers.}

% partitioning issue


% \subsection{IO Time}
% 
% 
% \subsection{Serialization}
% We think that serialization could have an important effect on the makespan of the
% application. This could potentially slow down the application when a lot of tasks are
% scheduled.
% 
% \subsection{NFS}
% The NFS seems to be a source of the bottleneck. We think that exploring another file
% system, like Lustre, could give us insight into the actual effect of the NFS on the
% application makespan. We also think that the NFS caching could play an effect on the
% task of different lengths.
% 
% \subsection{Scheduler}
% The Dask scheduler seems to have some bizarre behavior. It seems like it waits to
% schedule tasks. It could also be due to tasks being scheduled on other workers
% thus requiring data to be sent over the network; which would explain the stall on the
% worker. More work would be needed to investigate that issue.
% 
% 
% \subsection{Caching}
% We think that caching plays an effect on the results. As seen in the Gantt chart
% previously, some of the read tasks are significantly shorter than others while all
% blocks are of equal size.

\section*{Acknowledgment}

Mathieu Dugr\'e was funded by an Undergraduate Student Research Assistant award from
the National Science and Engineering Research Council of Canada. We warmly thank
Compute Canada and \TG{Westgrid?} for providing the cloud infrastructure used in
these experiments, and the McGill Center for Integrative Neuroscience for giving us
access to their cloud allocation.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,reference}

\end{document}
