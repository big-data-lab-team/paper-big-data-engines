\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{times}
\usepackage[binary-units=true]{siunitx}
\usepackage{latexsym}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=blue,filecolor=black,urlcolor=blue}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{caption}
\usepackage{subcaption}

%\setlength{\textfloatsep}{7pt}

\newcommand{\TG}[1]{\color{cyan}From Tristan: #1 \color{black}}
\newcommand{\MD}[1]{\color{magenta}From Mathieu: #1 \color{black}}
\newcommand{\VHS}[1]{\color{green}From Valerie: #1 \color{black}}

\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
  T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Impact of engines scheduler on the performance of Big Data pipelines in HPC}

\author{Mathieu Dugr\'e, Val\'erie Hayot-Sasson, Tristan Glatard\\
	Department of Computer Science and Software Engineering\\
	Concordia University, Montr\'eal, Qu\'ebec, Canada\\
	\{mathieu.dugre, valerie.hayot-sasson, tristan.glatard\}@concordia.ca
	\vspace*{0.8cm} % to avoid weird spacing of 1st page by Latex.
}

\maketitle

\begin{abstract}
	% TODO
\end{abstract}

\begin{IEEEkeywords}
	Performance, Scheduler, Big Data, Dask, Spark, Neuroimaging
\end{IEEEkeywords}
\MD{Discuss which keyword to use for the paper.}

\section{Introduction}

\section{Background}
\subsection{Lustre} % TODO

\subsection{Dask}
Dask is a Python-based Big Data engine with growing popularity in the scientific Python ecosystem.
To mitigate the data transfer bottleneck in Big Data workflows, Dask was designed with data locality and in-memory in mind.
Data locality, popularized by Map-Reduce \cite{dean2008mapreduce}, schedules tasks where the data reside.
In-memory computing minimize the overhead of transfering data to disk by keeping data in memory when possible.
Dask uses lazy evalutation to reduce unnecessary communication and computation.
The engine builds a dynamic graph prior to execution allowing to determine which task to compute.
Dask workflows can further reduce data transfer by using leveraging mulithreading whenever the application is not bounded by Python's GIL.
Fault-tolerance is achieved by recording of data lineage, the sequence of operations used on to modify the initial data.

Dask offers five data structures:
\href{https://docs.dask.org/en/latest/array.html}{Array},
\href{https://docs.dask.org/en/latest/bag.html}{Bag},
\href{https://docs.dask.org/en/latest/dataframe.html}{DataFrame},
\href{https://docs.dask.org/en/latest/delayed.html}{Delayed},
and \href{https://docs.dask.org/en/latest/futures.html}{Futures}.
The Dask Array offers a clone of NumPy API for distributed processing of large arrays.
A Dask Bag is a distributed collection of Python object that offers a programming abstraction similar to \href{https://toolz.readthedocs.io/en/latest/}{PyToolz}.
Dask Dataframe is a parallel composition of Pandas Dataframes used to process large amount of tabular data.
Dask Delayed offers an API for distributing abrirtary function that do not fit in the above frameworks.
Lastly, Dask Futures can also execute abrirtary functions, however, it launch computation immediately rather than lazily.
Dask modularity allow users to install only required components making it lightweigth.

In Dask a scheduler decides where and when to execute tasks using the Dask graph.
API operations generate multiple fine-coarsed tasks in the computation graph allwing an easier representatin of complex algorithms.

The Dask engine is compatible to numerous distributed schedulers, including YARN and Mesos.
Dask also provide its own \textit{Dask Distributed scheduler}.
Althought Dask is compatible with schedulers available in HPC clusters,
We chose to use Dask Distributed scheduler, to keep the environment balanced between the engines.

In Dask Distributed scheduler, a \textit{dask-scheduler} process admistrates the resource provided by \textit{dask-worker} in the cluster.
The scheduler receives jobs from clients and assigns tasks to available workers.
Dask scheduler use a FIFO (First-In-First-Out) job scheduling policy.
That is, completely process a branch of the Dask graph before proceding with the next one.

Dask offers multiple ways to deploy a cluster including: with SSH configs, Kubernetes, SLURM, PBS, and others.
For our experiments, we used the \href{https://jobqueue.dask.org/en/latest/generated/dask_jobqueue.SLURMCluster.html}{Dask SLURM cluster} API.

Dask \MD{Add version when after locking it for experiments.} was used.

\subsection{Apache Spark}
Apache Spark is a widely-used general pupose Big Data engine.
Like Dask, it aims at reducing data transfer cost by incorporating data locality and in-memory computing, and lazy evaluation.



Spark primary data structure is Resilient Distributed Dataset (RDD)\cite{RDD}, a fault-tolerant, parallel collection of data elements.
RDDs is the basis of the other Spark data structure: Datasets and DataFrames.
Datasets are similar to RDD but benefits from additional performance by leveraging the Spark SQL's optimized execution engine.
The DataFrame API are Datasets organized into named-columns and used to process tabular data.
While the DataFrame API is available in all supported language, DAtasets are limited to Sacla and Java.

Python is a common programming language in the scientific community.
This is particulary true in the neuroimaging field with numerous specialized Python libraries to operate on data.
While serialization from Python to Java create overhead, we found it to be minimal \cite{8943502}.
We chose to focus on PySpark API to have a more balance environment between the different engines and due to its suitability to neuroimaging.

\section{Methods}
\subsection{Infrastructure}
For our experiments we used the SlashBin cluster at Concordia University.
Each compute node has 2 16-cores CPUs,
\SI{256}{\giga\byte} memory, \MD{find out what's the memory speed and \#-channel supported.}
\SI{2.88}{\tera\byte} SSDs of mounted storage.
The nodes are interconnected with a dedicated \SI{10}{\giga\bit/\second} Ethernet connection.
Centos \MD{Verify version} and Linux kernel \MD{Verify kernel version} is installed on the nodes.

\MD{Write about the worker/leader setup once the dataset size is determined. Need to tune so that 2-3 blocks can fit in the excutors memmory}


\subsection{Dataset}
We used BigBrain\cite{Amunts:13}, a 3-D image of the human brain with voxel intensity ranging between 0 and 65,535.
We converted the blocks into the NIfTI format, a popular format in neuroimaging.
We left the NIfTI blocks uncompressed, resulting in a total data size of \SI{00}{\giga\byte}\MD{Verify the size of the dataset when Lustre back on}.
To evaluate the effect of block size, we resplit these blocks into 30, 125 and 750 blocks of \MD{size for each block segment}, using the \href{https://github.com/big-data-lab-team/sam}{sam} library.
	
We also used the dataset provided by the Consortium for Reliability and Reproducibility (\href{http://fcon_1000.projects.nitrc.org/indi/CoRR/html/}{CoRR}) \cite{zuo2014open}.
This dataset is freely available on \href{https://datasets.datalad.org/?dir=/corr/RawDataBIDS}{datalad}.
The entire dataset is \SI{408.4}{\giga\byte}, containing anatomical, diffusion and functional images of 1,397 subjects acquiredin 29 sites.
We used all 3,491 anatomical images, representing \SI{39}{\giga\byte} overall (\SI{11.17}{\mega\byte} per image on average).
	
\subsection{Applications}
\subsubsection{Increment}
We adapted the increment pipeline used in \cite{hayot2019performance}.
This synthetic application reads blocks of the BigBrain from Lustre and simulates computation by sleeping for a specified period.
To simulate intermediate results, we repeat the sleep process for a configurable amount of time.
We prevent data caching of the blocks by incrementing their voxels value by one after each sleep operation.
Finally, we write the resulting NIfTI image back to Lustre.
This application allows us to study the engines when their inputs are processed independently.
The map-only scenario of this application mimics the processing of multiple independent subjects in parallel.
\MD{TODO: Add the Figure for the application graph.}
	
\subsubsection{Multi-Increment}
Our second application is a revision of the increment pipeline.
A significant difference is that, at each iteration, it uses a random BigBrain block as the increment value. 
This change allows the multi-increment application to have inter-worker communication while remaining simplistic.
\MD{TODO: Add the Figure for the application graph.}
	
\subsubsection{Histogram}
As our third application, we calculate the histogram of the BigBrain image. 
It reads the BigBrain blocks from Lustre, calculates each intensity's frequency, and then writes the aggregated result back on Lustre.
This map-reduce application has a very high read overwrite ratio.
Moreover, this application requires data shuffling, although little. 
The amount of inter-worker communication is in-between the increment and multi-increment pipelines.
\MD{TODO: Add the Figure for the application graph.}
	
\subsubsection{Kmeans} % TODO
For our fourth application, we use Kmeans to cluster the BigBrain image by voxel intensity.
We set the number of clusters to 3 to segment the white and grey matter and the noise.
The application starts by reading the image blocks, combining all voxels in a 1-D array, and choosing initial centroids using the min, max, and intermediate values.
It assigns each voxel to the centroids it is the closest and updates each centroid by computing the average of the voxels associated with it. 
It repeats the assignment and update steps for a configurable amount of time. 
Finally, the voxels of the image blocks are classified and write back on the file system.
Updating the centroids involves a lot of data communication between the worker.
	
Spark and Dask's implementation differs slightly for this application to take advantage of the" most suited" API from both engines.
Spark implementation uses the Map-Reduce paradigm, while Dask implementation uses array programming.
\MD{TODO: Add the Figure for the application graph.}
	
	
\subsubsection{BIDS app example}
Our last application is BIDS App example: a neuroimaging pipeline to extract brain volume.
For this application, we use the CoRR dataset.
It extracts the brain volume of each participant, then computes the average for each group of participants.
Unlike the other application, BIDS App example is a command-line set in a Docker image (bids/example on DockerHub).
We converted the Docker image to a Singularity image for use in HPC environments, \MD{Cite paper on reason why this is done.}
using \href{https://hub.docker.com/r/singularityware/docker2singularity/tags/}{docker2singularity}
\MD{Need to discuss if we preload the image on the worker nodes.}
\MD{TODO: Add the Figure for the application graph.}
	
\subsection{Experiments} % TODO
	
\section{Results} % TODO
	
\section{Discussion} % TODO
	
\section{Conclusion} % TODO
	
\section*{Acknowledgment} % TODO
	
	
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,reference}
	
\end{document}
