\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{times}
\usepackage[binary-units=true]{siunitx}
\usepackage{latexsym}
\usepackage{hyperref}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=blue,filecolor=black,urlcolor=blue}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{caption}
\usepackage{subcaption}

%\setlength{\textfloatsep}{7pt}

\newcommand{\TG}[1]{\color{cyan}From Tristan: #1 \color{black}}
\newcommand{\MD}[1]{\color{magenta}From Mathieu: #1 \color{black}}
\newcommand{\VHS}[1]{\color{green}From Valerie: #1 \color{black}}

\algnewcommand\algorithmicforeach{\textbf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
  T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\begin{document}

\title{Performance comparison of Dask and Apache Spark on HPC systems}

\author{Mathieu Dugr\'e, Val\'erie Hayot-Sasson, Tristan Glatard\\
	Department of Computer Science and Software Engineering\\
	Concordia University, Montr\'eal, Qu\'ebec, Canada\\
	\{mathieu.dugre, valerie.hayot-sasson, tristan.glatard\}@concordia.ca
	\vspace*{0.8cm} % to avoid weird spacing of 1st page by Latex.
}

\maketitle

\begin{abstract}
	% TODO
\end{abstract}

\begin{IEEEkeywords}
	Performance, Big Data, Dask, Spark, Neuroimaging
\end{IEEEkeywords}
\MD{Discuss which keyword to use for the paper.}

\section{Introduction}

\section{Background}
\subsection{Lustre} % TODO

\subsection{Dask}
Dask is a Python-based Big Data engine with growing popularity in the scientific Python ecosystem.
Dask was designed with data locality and in-memory computing in mind, to mitigate the data transfer bottleneck in Big Data workflows.
Data locality, popularized by Map-Reduce \cite{dean2008mapreduce}, schedules tasks where the data reside.
In-memory computing minimizes the overhead of transferring data to disk by keeping data in memory when possible.
Dask uses lazy evaluation to reduce unnecessary communication and computation.
The engine builds a dynamic graph before execution, allowing it to determine which task to compute.
Dask workflows can further reduce data transfer by leveraging multithreading whenever Python's GIL does not restrict it.
Fault-tolerance is achieved by recording data lineage: the sequence of operations used to modify the initial data.

Dask offers five data structures:
\href{https://docs.dask.org/en/latest/array.html}{Array},
\href{https://docs.dask.org/en/latest/bag.html}{Bag},
\href{https://docs.dask.org/en/latest/dataframe.html}{DataFrame},
\href{https://docs.dask.org/en/latest/delayed.html}{Delayed},
and \href{https://docs.dask.org/en/latest/futures.html}{Futures}.
Arrays offer a clone of NumPy API for distributed processing of large arrays.
Bags are a distributed collection of Python object that offers a programming abstraction similar to \href{https://toolz.readthedocs.io/en/latest/}{PyToolz}.
Dataframes are a parallel composition of \href{https://pandas.pydata.org/}{Pandas} Dataframes used to process a large amount of tabular data.
Dask Delayed offers an API for distributing arbitrary functions that do not fit in the above frameworks.
Lastly, Dask Futures can also execute arbitrary functions; however, it launches computation immediately rather than lazily.
Dask modularity allows users to install only required components making it lightweight.

In Dask, a scheduler decides where and when to execute tasks using the Dask graph.
API operations generate multiple fine-coarse tasks in the computation graph, allowing a more straightforward representation of complex algorithms.

The Dask engine is compatible with multiple distributed schedulers, including YARN and Mesos.
Dask also provides its own \textit{Dask Distributed scheduler}.
We chose to use Dask Distributed scheduler to keep the environment balanced between the engines.

In the Dask Distributed scheduler, a \textit{dask-scheduler} process administrates the resource provided by  \textit{dask-worker}s in the cluster.
The scheduler receives jobs from clients and assigns tasks to available workers.
Task scheduler uses a LIFO (Last-In-First-Out) job scheduling policy.
That is an utter process branch of the Dask graph before proceeding with the next one.

Dask offers multiple ways to deploy a cluster, including, but not limited to, SSH configs, Kubernetes, SLURM, PBS.
For our experiments, we used the \href{https://jobqueue.dask.org/en/latest/generated/dask_jobqueue.SLURMCluster.html}{Dask SLURM cluster} API.

\subsection{Apache Spark}
Apache Spark is a widely-used general-purpose Big Data engine.
Like Dask, it aims at reducing data transfer costs by incorporating data locality, in-memory computing, and lazy evaluation.

Spark offers three options to schedule jobs: Spark Standalone, Mesos, and YARN.
Spark Standalone is a simple built-in scheduler.
YARN is mainly used to schedule Hadoop-based workflows, while Mesos can be used for various workflows.
We limit our focus to Spark Standalone scheduler, as researchers are likely to execute their workflows in an HPC environment where, usually, neither YARN nor Mesos is available.

In the Spark Standalone scheduler, a \textit{leader} \TG{I am all in favor of inclusive terminology, but if Spark didn't update their wording we shouldn't do it for them} coordinates the resource provisioned by \textit{workers} in the cluster.
A \textit{driver}  process receives jobs from clients and requests workers from the leader.
Jobs are divided into stages to be executed onto workers.
Each operation in a stage is represented by a high-level task in the computation graph.
Like Dask, Spark Standalone scheduler uses a LIFO policy to schedule tasks.
Spark Standalone has two execution modes: (1) the client mode, where the driver process runs in a dedicated process,
and (2) the cluster mode, where the driver runs within a worker process.
Our experiments use the client mode since cluster mode is not available in PySpark.

Spark's primary data structure is Resilient Distributed Dataset (RDD)\cite{RDD}, a fault-tolerant, parallel collection of data elements.
RDDs are the basis of the other Spark data structure: Datasets and DataFrames.
Datasets are similar to RDD but benefit additional performance by leveraging the Spark SQL's optimized execution engine. 
The DataFrames are Datasets organized into named-columns and are used to process tabular data. 
While the DataFrame API is available in all supported languages, Datasets are limited to Scala and Java. 

Python is a standard programming language in the scientific community, offering numerous data processing libraries.
While serialization from Python to Java, an operation required when using Spark's Python API, creates overhead, we found it minimal \cite{8943502}.
We focus on PySpark API to have a more balanced environment between the different engines and for its suitability to neuroimaging.

\section{Methods}
\subsection{Infrastructure}
We used the ``slashbin" cluster at Concordia University.
The cluster has 8 compute nodes, 4 storage nodes, 1 login node, and 1
Lustre metadata node. Each
compute node has 2 $\times$ 16-core Intel(R) Xeon(R) Gold 6130 CPU
@ 2.10GHz, 256~GiB of RAM, 6 $\times$ SSDs of 450~GiB each with the XFS
file system (no RAID enabled), 378~GiB of tmpfs, 126~GiB of devtmpfs file system,
CentOS~8.1 and Linux kernel
\textit{4.18.0-240.1.1.el8\_lustre.x86\_64}.

\TG{describe the Lustre installation: data nodes and lustre config}
\TG{Use GB or GiB consistently}

Both Spark and Dask were configured to have 8 worker processes each with 8
threads. Each worker was allocated \SI{31.5}{\giga\byte} of memory \TG{,
determined as the amount of memory that ...}. \TG{any other Spark or Dask
param? Location of log files, etc} A new Dask or Spark cluster was started and
teared down for each experiment.

Dask ?? and Spark ?? \MD{Add version when after locking it for experiments.} was used for our experiments.

\subsection{Dataset}
We used BigBrain\cite{Amunts:13}, a 3-D image of the human brain with voxel
intensities ranging from 0 to 65,535. We converted the blocks into the
NIfTI format, a popular format in neuroimaging. We left the NIfTI blocks
uncompressed, resulting in a total data size of \SI{648}{\giga\byte}. To
evaluate the effect of block size, we resplit these blocks into 1000, 2500,
and 5000 blocks of \SI{648}{\mega\byte}, \SI{259.2}{\mega\byte}, and
\SI{129.6}{\mega\byte}, respectively. The
\href{https://github.com/big-data-lab-team/sam}{sam} library was used to
resplit the image.

We also used the dataset provided by the Consortium for Reliability and
Reproducibility
(\href{http://fcon_1000.projects.nitrc.org/indi/CoRR/html/}{CoRR})
\cite{zuo2014open}, freely available on
\href{https://datasets.datalad.org/?dir=/corr/RawDataBIDS}{DataLad}. The
entire dataset is \SI{408.4}{\giga\byte}, containing anatomical, diffusion
and functional images of 1,397 subjects acquired in 29 sites. We used all
3,491 anatomical images, representing \SI{39}{\giga\byte} overall
(\SI{11.17}{\mega\byte} per image on average).

\subsection{Applications}

\subsubsection{Increment}

We adapted the increment application used in \cite{hayot2019performance}.
This synthetic application reads blocks of the BigBrain from Lustre and
simulates computation by sleeping for a specified period. To simulate
intermediate results, we repeat the sleep process for a configurable amount
of time. We prevent data caching of the blocks by incrementing their voxels
value by one after each sleep operation. Finally, we write the resulting
NIfTI image back to Lustre. This application allows us to study the engines
when their inputs are processed independently. The map-only pattern
 mimics the processing of multiple independent subjects in
parallel.

\begin{figure}[!ht]
	\centering
	\includegraphics[height=\columnwidth,
	angle=0]{figures/increment.png}
	\caption{Task graph for Incrementation with 3 iterations and 3 BigBrain blocks.
	\TG{nitpick: I'd prefer using ``read and write" than ``load and dump''}}
	\label{fig:graph-increment}
\end{figure}

% Currently ommitted due to issue with memory.
% 
\subsubsection{Multi-Increment}
Our second application is an adaptation of the increment application. A
significant difference is that, at each iteration, it uses a random
BigBrain block as the increment value. This change allows the
multi-increment application to have inter-worker communication while
remaining simple.

\begin{figure}[!hb]
	\centering
	\includegraphics[height=\columnwidth,
	angle=0]{figures/multi-increment.png}
	\caption{Task graph for Multi-Incrementation with 3 iterations and 3 BigBrain blocks.}
	\label{fig:graph-muti-increment}
\end{figure}
	
\subsubsection{Histogram}
As our third application, we calculate the histogram of the BigBrain image. 
The application reads the BigBrain blocks from Lustre, calculates each intensity's frequency, and then writes the aggregated result back on Lustre.
This map-reduce application has a very high read overwrite ratio.
Moreover, this application requires shuffling, albeit of a limited amount of data. 
The amount of inter-worker communication is in-between the increment and multi-increment applications.

\begin{figure}[!hb]
	\centering
	\includegraphics[height=\columnwidth,
	angle=0]{figures/histogram.png}
	\caption{Task graph for Histogram with 3 BigBrain blocks.}
	\label{fig:graph-histogram}
\end{figure}
	
\subsubsection{Kmeans}
For our fourth application, we apply Kmeans clustering to the voxel
intensities of the BigBrain image. We set the number of clusters to 3. The
application starts by reading the image blocks, combining all voxels in a
1-D array, and choosing initial centroids using the min, max, and
intermediate values. It assigns each voxel to its closest centroid and
updates each centroid with the average of its assigned voxels. It repeats
the assignment and update steps for a configurable number of iterations.
Finally, the voxels of the image blocks are classified and written back to
the file system. Updating the centroids involves substantial data
communication between the workers.

For this application, the Spark and Dask implementations differ slightly,
to take advantage of the best-suited API from both engines. The Spark
implementation uses the Map-Reduce paradigm, while the Dask one uses array
programming.
	
	
\subsubsection{BIDS App example}
Our fifth application is BIDS App example, a neuroimaging pipeline to
measure the brain volume from MRIs. For this application, we use the CoRR
dataset. The application extracts the brain volume of each participant,
then computes the average for each group of participants. Unlike the other
applications, BIDS App example is a command-line executed in a Docker image
(bids/example on DockerHub). We converted the Docker image to a Singularity
image for use in HPC environments, \MD{Cite paper on reason why this is
done.} using
\href{https://hub.docker.com/r/singularityware/docker2singularity/tags/}{docker2singularity}

\subsubsection{BIDS App MRIQC}
Our last application is BIDS App MRIQC: a neuroimaging pipeline to perfom quality control of a brain image.
For this application, we use the CoRR dataset. \MD{Validate if this one was used after running the experiments. Maybe another dataset will be required; e.g. ADHD200}
This application verifies the quality of images on a per-sbuject basis.
Like the BIDS App example, this application is runned using a command-line tool.
We use the same method to convert the Docker image to a Singularity image.

\TG{You should explain how the applications were implemented in Dask and
Spark, that you reused the same code in both  cases, which APIs were used 
and why, etc}

\subsection{Experiments}
Table~\ref{table:parameters} shows the parameters that were varied
throughout the experiments. We varied (1) the number of workers to assess
the scalibility of the engine scheduler, (2) the BigBrain
block size in Increment, Multi-Increment, and Histogram to measure the
effect of different I/O patterns and parallelization degrees, (3) the
number of iterations to evaluate the effect of number of task, and (4) the
sleep delay to study the effect of task duration. It should note that
increasing the number of iterations for a given sleep delay also increases
the total compute time of an application.

To avoid potential external biases due to caching, background processes and
network load, we ran the applications in randomized order and cleared the
page cache in between every experiments. Each benchmark was run ten times.

For each run, we measured the makespan of the application as well as the
cumulative time spent in the different functions for read, processing, and
writing data. The overhead calculation for each CPU thread is the end time
of the last processed task minus the total runtime of the tasks ran for
this thread \TG{unclear definition. Don't refer to CPU threads as a given task may be executed on more than 1. Maybe refer to engine executors?}. Summing those results gives the total overhead for the
application.

\begin{table*}[t]
	\renewcommand{\arraystretch}{1.5}
	\caption{Parameters for the experiments}\label{table:parameters}
	\centering
	\begin{tabular}{|l|c|c|c|c|c|c|}
		\hline & Increment & Multi-Increment & Kmeans & Histogram & BIDS Example & BIDS MRIQC \\\hline
		\# of Nodes & \multicolumn{6}{c|}{2, 4, 8} \\\hline
		\# of Workers & \multicolumn{6}{c|}{16, 32 , 64} \\ \hline
		Block resplit & \multicolumn{4}{c|}{1000, 2500, 5000}  & \multicolumn{2}{c|}{n/a} \\\hline
		Block Size {[}\SI{}{\mega\byte}{]} & \multicolumn{4}{c|}{648, 259.2, 129.6} & \multicolumn{2}{c|}{n/a} \\\hline
		% \# of Subjects & \multicolumn{4}{c|}{n/a} & \multicolumn{2}{c|}{1397} \\ \hline
		\# of Iterations & \multicolumn{3}{c|}{1, 8, 64}                 & \multicolumn{3}{c|}{n/a} \\\hline
		Sleep Delay {[}\SI{}{\second}{]} & \multicolumn{2}{c|}{0.25, 1, 4, 16} & \multicolumn{4}{c|}{n/a} \\\hline
	\end{tabular}
\end{table*}
\MD{Decide which row to keep for (\# fo nodes / \# of workers) and (Block resplit / Block size)}

\section{Results} % TODO
\section{Discussion} % TODO
\section{Conclusion} % TODO
\section*{Acknowledgment} % TODO

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,reference}
\end{document}
